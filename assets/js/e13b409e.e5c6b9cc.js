"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[4058],{18587:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>o,contentTitle:()=>s,default:()=>c,frontMatter:()=>r,metadata:()=>l,toc:()=>p});var a=i(85893),n=i(11151);const r={description:"Hyperparameter Optimization with\xa0Optuna",tags:["Optuna","Hyperparameter Optimization","Machine Learning","Data Science","Python"]},s="Optuna",l={id:"hypertune/optuna",title:"Optuna",description:"Hyperparameter Optimization with\xa0Optuna",source:"@site/docs/hypertune/optuna.md",sourceDirName:"hypertune",slug:"/hypertune/optuna",permalink:"/portfolio/docs/hypertune/optuna",draft:!1,unlisted:!1,tags:[{label:"Optuna",permalink:"/portfolio/docs/tags/optuna"},{label:"Hyperparameter Optimization",permalink:"/portfolio/docs/tags/hyperparameter-optimization"},{label:"Machine Learning",permalink:"/portfolio/docs/tags/machine-learning"},{label:"Data Science",permalink:"/portfolio/docs/tags/data-science"},{label:"Python",permalink:"/portfolio/docs/tags/python"}],version:"current",frontMatter:{description:"Hyperparameter Optimization with\xa0Optuna",tags:["Optuna","Hyperparameter Optimization","Machine Learning","Data Science","Python"]},sidebar:"tutorialSidebar",previous:{title:"kerasTuner",permalink:"/portfolio/docs/hypertune/kerasTuner"},next:{title:"Machine learning",permalink:"/portfolio/docs/machine-learning/"}},o={},p=[{value:"Introduction",id:"introduction",level:2},{value:"What Are Hyperparameters?",id:"what-are-hyperparameters",level:3},{value:"The Impact of Hyperparameters",id:"the-impact-of-hyperparameters",level:3},{value:"Why Optuna?",id:"why-optuna",level:3},{value:"Setting Up Your Environment",id:"setting-up-your-environment",level:2},{value:"Installing Optuna and dependencies",id:"installing-optuna-and-dependencies",level:3},{value:"Importing Necessary Libraries",id:"importing-necessary-libraries",level:3},{value:"Loading Your\xa0Dataset",id:"loading-yourdataset",level:3},{value:"Defining Your Objective Function",id:"defining-your-objective-function",level:2},{value:"Crafting the Objective Function",id:"crafting-the-objective-function",level:3},{value:"The Magic of Optuna&#39;s study\xa0Object",id:"the-magic-of-optunas-studyobject",level:3},{value:"Exploring Search\xa0Spaces",id:"exploring-searchspaces",level:2},{value:"Continuous vs. Discrete Hyperparameters",id:"continuous-vs-discrete-hyperparameters",level:3},{value:"Customizing Your Search\xa0Space",id:"customizing-your-searchspace",level:3},{value:"Optimization Algorithms",id:"optimization-algorithms",level:2},{value:"TPE (Tree-structured Parzen Estimator)",id:"tpe-tree-structured-parzen-estimator",level:3},{value:"Random Search (Sometimes Simplicity Wins)",id:"random-search-sometimes-simplicity-wins",level:3},{value:"Trials, Trials,\xa0Trials!",id:"trials-trialstrials",level:2},{value:"Running Your Optimization Trials",id:"running-your-optimization-trials",level:3},{value:"Visualizing Results",id:"visualizing-results",level:2},{value:"Tracking Progress",id:"tracking-progress",level:3},{value:"Plotting Hyperparameter Distributions",id:"plotting-hyperparameter-distributions",level:3},{value:"Scatter Plots, Parallel Coordinates, and\xa0More",id:"scatter-plots-parallel-coordinates-andmore",level:3},{value:"Stay Curious, Fellow Starfarers!",id:"stay-curious-fellow-starfarers",level:3},{value:"Conclusion and Next\xa0Steps",id:"conclusion-and-nextsteps",level:2}];function h(e){const t={code:"code",h1:"h1",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,n.a)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.h1,{id:"optuna",children:"Optuna"}),"\n",(0,a.jsx)(t.p,{children:"In this hands-on tutorial, we'll dive deep into the fascinating world of hyperparameter optimization using Optuna, a powerful Python library. Buckle up, because we're about to embark on a journey that will elevate your machine learning models to new heights!"}),"\n",(0,a.jsx)(t.h2,{id:"introduction",children:"Introduction"}),"\n",(0,a.jsx)(t.p,{children:"What exactly are hyperparameters, and why should you care? Let's unravel this mystery."}),"\n",(0,a.jsx)(t.h3,{id:"what-are-hyperparameters",children:"What Are Hyperparameters?"}),"\n",(0,a.jsx)(t.p,{children:"Think of hyperparameters as the knobs and switches on your model's control panel. They don't get learned during training (unlike model weights), but they significantly impact your model's performance. Examples include learning rates, batch sizes, regularization strengths, and the number of hidden layers in a neural network."}),"\n",(0,a.jsx)(t.h3,{id:"the-impact-of-hyperparameters",children:"The Impact of Hyperparameters"}),"\n",(0,a.jsx)(t.p,{children:"Imagine you're baking a cake. The ingredients (data) are essential, but how long you bake it (hyperparameters) determines whether it's a fluffy masterpiece or a burnt disaster. Similarly, choosing the right hyperparameters can make or break your model's accuracy, convergence speed, and generalization ability."}),"\n",(0,a.jsx)(t.h3,{id:"why-optuna",children:"Why Optuna?"}),"\n",(0,a.jsx)(t.p,{children:"Now, let's talk about Optuna! This Python library intelligently explores the hyperparameter space, finds the sweet spots, and optimizes your model with minimal effort."}),"\n",(0,a.jsx)(t.h2,{id:"setting-up-your-environment",children:"Setting Up Your Environment"}),"\n",(0,a.jsx)(t.p,{children:"Let's roll up our sleeves and get our environment ready for some hyperparameter magic."}),"\n",(0,a.jsx)(t.h3,{id:"installing-optuna-and-dependencies",children:"Installing Optuna and dependencies"}),"\n",(0,a.jsx)(t.p,{children:"First things first, let's install Optuna. Open your terminal (or Anaconda prompt) and type:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-bash",metastring:'title="Shell"',children:"pip install optuna\n"})}),"\n",(0,a.jsx)(t.h3,{id:"importing-necessary-libraries",children:"Importing Necessary Libraries"}),"\n",(0,a.jsx)(t.p,{children:"Next, fire up your favorite Python IDE (mine's VS Code). Import the following libraries:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",metastring:'title="Python"',children:"import optuna\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\nSEED = 77\n"})}),"\n",(0,a.jsx)(t.h3,{id:"loading-yourdataset",children:"Loading Your\xa0Dataset"}),"\n",(0,a.jsx)(t.p,{children:"For this tutorial, we'll use a classic dataset\u200a-\u200athe Iris dataset. It teaches us about flower species. If you don't have it already, fetch it using:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",metastring:'title="Python"',children:"iris = load_iris()\nX, y = iris.data, iris.target\n"})}),"\n",(0,a.jsx)(t.p,{children:"Now you're all set! Your environment is primed, and the data is at your fingertips."}),"\n",(0,a.jsx)(t.h2,{id:"defining-your-objective-function",children:"Defining Your Objective Function"}),"\n",(0,a.jsx)(t.p,{children:"Welcome to the heart of hyperparameter optimization! Here, we'll craft the function we want to optimize."}),"\n",(0,a.jsx)(t.h3,{id:"crafting-the-objective-function",children:"Crafting the Objective Function"}),"\n",(0,a.jsx)(t.p,{children:"Our objective function takes hyperparameters as inputs and returns a score (or loss) that we want to minimize or maximize. For example, in a classification task, it could be the cross-entropy loss or accuracy."}),"\n",(0,a.jsx)(t.p,{children:"Let's say we're training a random forest classifier on the Iris dataset. Our training and test dataset looks like this:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",metastring:'title="Python"',children:'X_train, X_val, y_train, y_val = train_test_split(\n    X, y, test_size=0.2, random_state=SEED\n)\n\n# The first 3 elements\nprint(f"X_train:\\n{X_train[:3]}...\\n")\nprint(f"y_train:\\n{y_train[:3]}...\\n")\nprint(f"X_val:\\n{X_val[:3]}...\\n")\nprint(f"y_val:\\n{y_val[:3]}...\\n")\n'})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"X_train:\n[[6.3 2.5 4.9 1.5]\n [7.2 3.  5.8 1.6]\n [5.3 3.7 1.5 0.2]]...\n\ny_train:\n[1 2 0]...\n\nX_val:\n[[5.8 2.7 3.9 1.2]\n [6.3 2.8 5.1 1.5]\n [5.7 2.5 5.  2. ]]...\n\ny_val:\n[1 2 2]...\n"})}),"\n",(0,a.jsx)(t.p,{children:"Our objective function might look like this:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",metastring:'title="Python"',children:'def objective(trial):\n    # Hyperparameters to optimize\n    n_estimators = trial.suggest_int("n_estimators", 10, 100)\n    max_depth = trial.suggest_int("max_depth", 3, 20)\n    min_samples_split = trial.suggest_float("min_samples_split", 0.1, 1.0)\n    \n    # Create and train the model\n    model = RandomForestClassifier(\n        n_estimators=n_estimators,\n        max_depth=max_depth,\n        min_samples_split=min_samples_split,\n        random_state=SEED\n    )\n    model.fit(X_train, y_train)\n    \n    # Evaluate on validation set\n    val_preds = model.predict(X_val)\n    val_accuracy = accuracy_score(y_val, val_preds)\n    \n    # Our goal: maximize accuracy!\n    return val_accuracy\n'})}),"\n",(0,a.jsx)(t.h3,{id:"the-magic-of-optunas-studyobject",children:"The Magic of Optuna's study\xa0Object"}),"\n",(0,a.jsx)(t.p,{children:"Now, let's summon the Optuna study! This object orchestrates our optimization process. It explores the hyperparameter space, evaluates our objective function, and guides us toward the best configuration."}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",metastring:'title="Python"',children:'study = optuna.create_study(direction="maximize")  # Maximize accuracy\n'})}),"\n",(0,a.jsx)(t.h2,{id:"exploring-searchspaces",children:"Exploring Search\xa0Spaces"}),"\n",(0,a.jsx)(t.p,{children:"In this section, we'll delve into different search spaces, where each dimension represents a hyperparameter, and each point holds the promise of better model performance."}),"\n",(0,a.jsx)(t.h3,{id:"continuous-vs-discrete-hyperparameters",children:"Continuous vs. Discrete Hyperparameters"}),"\n",(0,a.jsx)(t.p,{children:"Imagine our search space as a cosmic playground. Some hyperparameters roam freely in continuous realms, while others hop discretely from one value to another. Let's break it down:"}),"\n",(0,a.jsxs)(t.ol,{children:["\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Continuous Hyperparameters:"})," These are real-valued and infinite. Examples include learning rates, dropout probabilities, and weight decay coefficients. Optuna glides through these spaces, sampling points."]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Discrete Hyperparameters:"})," Think of these as integer-valued or categorical. Batch sizes, tree depths, and activation functions fall into this category."]}),"\n"]}),"\n",(0,a.jsx)(t.h3,{id:"customizing-your-searchspace",children:"Customizing Your Search\xa0Space"}),"\n",(0,a.jsx)(t.p,{children:"You have the power to define your own search space. Set bounds, constraints, and explore the uncharted. Whether you're seeking optimal hyperparameters or performance, Optuna will guide you."}),"\n",(0,a.jsx)(t.h2,{id:"optimization-algorithms",children:"Optimization Algorithms"}),"\n",(0,a.jsx)(t.p,{children:"In this section, we'll unveil the guides that navigate our hyperparameters pointing us toward optimal configurations."}),"\n",(0,a.jsx)(t.h3,{id:"tpe-tree-structured-parzen-estimator",children:"TPE (Tree-structured Parzen Estimator)"}),"\n",(0,a.jsx)(t.p,{children:"TPE balances exploration and exploitation. It narrows down the search space, focusing on promising regions."}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",metastring:'title="Python"',children:'study_tpe = optuna.create_study(\n    direction="maximize", sampler=optuna.samplers.TPESampler()\n)\n'})}),"\n",(0,a.jsx)(t.h3,{id:"random-search-sometimes-simplicity-wins",children:"Random Search (Sometimes Simplicity Wins)"}),"\n",(0,a.jsx)(t.p,{children:"Random search explores unpredictably. While not as sophisticated as TPE, it surprises us with simplicity."}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",metastring:'title="Python"',children:'study_random = optuna.create_study(\n    direction="maximize", sampler=optuna.samplers.RandomSampler()\n)\n'})}),"\n",(0,a.jsx)(t.h2,{id:"trials-trialstrials",children:"Trials, Trials,\xa0Trials!"}),"\n",(0,a.jsx)(t.p,{children:"In this section, we'll unleash our hyperparameter trials. Each trial is a chance to uncover hidden constellations of optimal configurations."}),"\n",(0,a.jsx)(t.h3,{id:"running-your-optimization-trials",children:"Running Your Optimization Trials"}),"\n",(0,a.jsx)(t.p,{children:"Optuna orchestrates our trials, tweaking hyperparameters, training models, and evaluating their performance."}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",metastring:'title="Python"',children:"# Let the trials begin!\nstudy.optimize(objective, n_trials=100)\n"})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"[I 2024-02-12 11:56:10,802] A new study created in memory with name: no-name-8134c441-bdcf-452a-9ed5-ae0c8722c345\n[I 2024-02-12 11:56:10,877] Trial 0 finished with value: 0.8666666666666667 and parameters: {'n_estimators': 65, 'max_depth': 12, 'min_samples_split': 0.3146680813827415}. Best is trial 0 with value: 0.8666666666666667.\n[I 2024-02-12 11:56:10,968] Trial 1 finished with value: 0.3 and parameters: {'n_estimators': 93, 'max_depth': 11, 'min_samples_split': 0.7188358544224039}. Best is trial 0 with value: 0.8666666666666667.\n[I 2024-02-12 11:56:11,068] Trial 2 finished with value: 0.8666666666666667 and parameters: {'n_estimators': 97, 'max_depth': 3, 'min_samples_split': 0.17517568716403698}. Best is trial 0 with value: 0.8666666666666667.\n...\n[I 2024-02-12 11:56:19,256] Trial 97 finished with value: 0.8666666666666667 and parameters: {'n_estimators': 97, 'max_depth': 8, 'min_samples_split': 0.284594211567275}. Best is trial 22 with value: 0.9.\n[I 2024-02-12 11:56:19,332] Trial 98 finished with value: 0.8666666666666667 and parameters: {'n_estimators': 59, 'max_depth': 6, 'min_samples_split': 0.38599015615782106}. Best is trial 22 with value: 0.9.\n[I 2024-02-12 11:56:19,405] Trial 99 finished with value: 0.8666666666666667 and parameters: {'n_estimators': 54, 'max_depth': 11, 'min_samples_split': 0.16165709051161997}. Best is trial 22 with value: 0.9.\n"})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",metastring:'title="Python"',children:"# TPE\nstudy_tpe.optimize(objective, n_trials=100)\n"})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"[I 2024-02-12 11:50:04,668] A new study created in memory with name: no-name-406e0cce-60e6-4420-8aeb-82c4bef5c2a8\n[I 2024-02-12 11:50:04,735] Trial 0 finished with value: 0.26666666666666666 and parameters: {'n_estimators': 37, 'max_depth': 7, 'min_samples_split': 0.7885613884209568}. Best is trial 0 with value: 0.26666666666666666.\n[I 2024-02-12 11:50:04,754] Trial 1 finished with value: 0.26666666666666666 and parameters: {'n_estimators': 15, 'max_depth': 5, 'min_samples_split': 0.6731251336910595}. Best is trial 0 with value: 0.26666666666666666.\n[I 2024-02-12 11:50:04,849] Trial 2 finished with value: 0.3 and parameters: {'n_estimators': 86, 'max_depth': 12, 'min_samples_split': 0.7460270374607401}. Best is trial 2 with value: 0.3.\n...\n[I 2024-02-12 11:50:13,359] Trial 97 finished with value: 0.8666666666666667 and parameters: {'n_estimators': 72, 'max_depth': 10, 'min_samples_split': 0.2193760150748684}. Best is trial 7 with value: 0.8666666666666667.\n[I 2024-02-12 11:50:13,444] Trial 98 finished with value: 0.8666666666666667 and parameters: {'n_estimators': 67, 'max_depth': 18, 'min_samples_split': 0.27915755394957986}. Best is trial 7 with value: 0.8666666666666667.\n[I 2024-02-12 11:50:13,518] Trial 99 finished with value: 0.8333333333333334 and parameters: {'n_estimators': 57, 'max_depth': 7, 'min_samples_split': 0.4796829947804756}. Best is trial 7 with value: 0.8666666666666667.\n"})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",metastring:'title="Python"',children:"# Random search\nstudy_random.optimize(objective, n_trials=100)\n"})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"[I 2024-02-12 11:54:04,344] A new study created in memory with name: no-name-6a1f6b6b-86c3-480a-ae3b-ae341ab5fca1\n[I 2024-02-12 11:54:04,370] Trial 0 finished with value: 0.8333333333333334 and parameters: {'n_estimators': 18, 'max_depth': 3, 'min_samples_split': 0.5323455027233865}. Best is trial 0 with value: 0.8333333333333334.\n[I 2024-02-12 11:54:04,448] Trial 1 finished with value: 0.8333333333333334 and parameters: {'n_estimators': 75, 'max_depth': 7, 'min_samples_split': 0.5407936894092948}. Best is trial 0 with value: 0.8333333333333334.\n[I 2024-02-12 11:54:04,499] Trial 2 finished with value: 0.8666666666666667 and parameters: {'n_estimators': 47, 'max_depth': 17, 'min_samples_split': 0.6854362685752783}. Best is trial 2 with value: 0.8666666666666667.\n...\n[I 2024-02-12 11:54:10,189] Trial 97 finished with value: 0.8666666666666667 and parameters: {'n_estimators': 86, 'max_depth': 5, 'min_samples_split': 0.17671758874989624}. Best is trial 62 with value: 0.9333333333333333.\n[I 2024-02-12 11:54:10,242] Trial 98 finished with value: 0.8666666666666667 and parameters: {'n_estimators': 45, 'max_depth': 16, 'min_samples_split': 0.5891751430666154}. Best is trial 62 with value: 0.9333333333333333.\n[I 2024-02-12 11:54:10,276] Trial 99 finished with value: 0.26666666666666666 and parameters: {'n_estimators': 30, 'max_depth': 10, 'min_samples_split': 0.9734243825309358}. Best is trial 62 with value: 0.9333333333333333.Stay Curious!\n"})}),"\n",(0,a.jsx)(t.p,{children:"Our trials are like shooting arrows into the unknown. Some hit bullseyes, others graze distant planets. But with each trial, we inch closer to the optimal hyperparameters that unlock our model's potential."}),"\n",(0,a.jsx)(t.h2,{id:"visualizing-results",children:"Visualizing Results"}),"\n",(0,a.jsx)(t.p,{children:"In this section, we'll create visualizations (for TPE study) that reveal the essence of our hyperparameter optimization journey."}),"\n",(0,a.jsx)(t.h3,{id:"tracking-progress",children:"Tracking Progress"}),"\n",(0,a.jsx)(t.p,{children:"As our trials unfold, Optuna records their trajectories. You can visualize it using Optuna:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",metastring:'title="Python"',children:"optuna.visualization.plot_optimization_history(study_tpe)\n"})}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:i(9247).Z+"",width:"692",height:"450"})}),"\n",(0,a.jsx)(t.p,{children:"The plot reveals peaks, valleys, and the path to enlightenment."}),"\n",(0,a.jsx)(t.h3,{id:"plotting-hyperparameter-distributions",children:"Plotting Hyperparameter Distributions"}),"\n",(0,a.jsx)(t.p,{children:"Imagine a canvas splashed with colors\u200a-\u200aeach hue representing a hyperparameter. Optuna offers beautiful plots to visualize their distributions:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",metastring:'title="Python"',children:"optuna.visualization.plot_param_importances(study_tpe)\n"})}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:i(46426).Z+"",width:"692",height:"450"})}),"\n",(0,a.jsx)(t.p,{children:"This plot highlights the most influential hyperparameters."}),"\n",(0,a.jsx)(t.h3,{id:"scatter-plots-parallel-coordinates-andmore",children:"Scatter Plots, Parallel Coordinates, and\xa0More"}),"\n",(0,a.jsx)(t.p,{children:"Our data points dance across dimensions. Optuna's scatter plots and parallel coordinates reveal their choreography:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",metastring:'title="Python"',children:"optuna.visualization.plot_parallel_coordinate(study_tpe)\n"})}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:i(53668).Z+"",width:"692",height:"450"})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",metastring:'title="Python"',children:"optuna.visualization.plot_slice(study_tpe)\n"})}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:i(94699).Z+"",width:"692",height:"450"})}),"\n",(0,a.jsx)(t.p,{children:"These visualizations connect the dots, showing how hyperparameters interact."}),"\n",(0,a.jsx)(t.h3,{id:"stay-curious-fellow-starfarers",children:"Stay Curious, Fellow Starfarers!"}),"\n",(0,a.jsx)(t.p,{children:"Our visualizations are like telescopes\u200a-\u200arevealing patterns beyond raw numbers."}),"\n",(0,a.jsx)(t.h2,{id:"conclusion-and-nextsteps",children:"Conclusion and Next\xa0Steps"}),"\n",(0,a.jsx)(t.p,{children:"As you navigate the hyperparameter space, experiment, iterate, and embrace uncertainty. There's no one-size-fits-all solution. Each dataset, each model, is a unique constellation waiting to be understood."}),"\n",(0,a.jsx)(t.p,{children:"Our journey doesn't end here. Explore Bayesian optimization, ensemble methods, and multi-objective optimization. Remember, there's always more to discover."})]})}function c(e={}){const{wrapper:t}={...(0,n.a)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(h,{...e})}):h(e)}},9247:(e,t,i)=>{i.d(t,{Z:()=>a});const a=i.p+"assets/images/optuna-history-dc04d0abf07b55619f1fe337bd71feb7.png"},53668:(e,t,i)=>{i.d(t,{Z:()=>a});const a=i.p+"assets/images/optuna-parallel-coordinate-d713d6e0fa641b59a7792938a387c3b1.png"},46426:(e,t,i)=>{i.d(t,{Z:()=>a});const a=i.p+"assets/images/optuna-param-importances-68f29f8a517eaa23696e34acca90e989.png"},94699:(e,t,i)=>{i.d(t,{Z:()=>a});const a=i.p+"assets/images/optuna-slice-bca060e6120911efd6112a91dd540197.png"},11151:(e,t,i)=>{i.d(t,{Z:()=>l,a:()=>s});var a=i(67294);const n={},r=a.createContext(n);function s(e){const t=a.useContext(r);return a.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function l(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:s(e.components),a.createElement(r.Provider,{value:t},e.children)}}}]);