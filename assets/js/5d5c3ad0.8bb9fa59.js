"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[8348],{11073:(e,n,r)=>{r.d(n,{A:()=>a});const a=r.p+"assets/images/kerastuner-hparams-table-14d8fa9754300320c4cf866e60291620.png"},28453:(e,n,r)=>{r.d(n,{R:()=>i,x:()=>o});var a=r(96540);const t={},s=a.createContext(t);function i(e){const n=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:i(e.components),a.createElement(s.Provider,{value:n},e.children)}},32348:(e,n,r)=>{r.d(n,{A:()=>a});const a=r.p+"assets/images/kerastuner-hparams-parallel-029a317c8935c3f26b437fbed9dc1758.png"},44500:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>c,frontMatter:()=>i,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"hypertune/kerasTuner","title":"kerasTuner","description":"How to use KerasTuner to tune hyperparameters","source":"@site/docs/hypertune/kerasTuner.md","sourceDirName":"hypertune","slug":"/hypertune/kerasTuner","permalink":"/portfolio/docs/hypertune/kerasTuner","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"Keras Tuner","permalink":"/portfolio/docs/tags/keras-tuner"},{"inline":true,"label":"Hyperparameter Tuning","permalink":"/portfolio/docs/tags/hyperparameter-tuning"},{"inline":true,"label":"Machine Learning","permalink":"/portfolio/docs/tags/machine-learning"},{"inline":true,"label":"Data Science","permalink":"/portfolio/docs/tags/data-science"},{"inline":true,"label":"Neural Networks","permalink":"/portfolio/docs/tags/neural-networks"}],"version":"current","frontMatter":{"description":"How to use KerasTuner to tune hyperparameters","tags":["Keras Tuner","Hyperparameter Tuning","Machine Learning","Data Science","Neural Networks"]},"sidebar":"tutorialSidebar","previous":{"title":"Hyperparameter Tuning","permalink":"/portfolio/docs/hypertune/"},"next":{"title":"Optuna","permalink":"/portfolio/docs/hypertune/optuna"}}');var t=r(74848),s=r(28453);const i={description:"How to use KerasTuner to tune hyperparameters",tags:["Keras Tuner","Hyperparameter Tuning","Machine Learning","Data Science","Neural Networks"]},o="kerasTuner",l={},d=[{value:"Introduction",id:"introduction",level:2},{value:"Getting KerasTuner Up and Running",id:"getting-kerastuner-up-and-running",level:2},{value:"Using KerasTuner on a Classification Problem",id:"using-kerastuner-on-a-classification-problem",level:2},{value:"Import Libraries",id:"import-libraries",level:3},{value:"Load and Preprocess the Data",id:"load-and-preprocess-the-data",level:3},{value:"Building the Model",id:"building-the-model",level:3},{value:"Defining the Tuner",id:"defining-the-tuner",level:3},{value:"Hyperparameter Search",id:"hyperparameter-search",level:3},{value:"Best Model",id:"best-model",level:3},{value:"Summary of Search Results",id:"summary-of-search-results",level:3},{value:"Evaluation",id:"evaluation",level:3},{value:"Analysing the Results",id:"analysing-the-results",level:3},{value:"Pro Tips for Mastering KerasTuner",id:"pro-tips-for-mastering-kerastuner",level:2}];function h(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"kerastuner",children:"kerasTuner"})}),"\n",(0,t.jsx)(n.p,{children:"Hello, fellow data enthusiasts! Today, we're going to embark on an exciting journey through the world of hyperparameters in machine learning. If you're like me, you probably love the thrill of building a machine learning model and watching it learn and improve. But have you ever wondered how we can make our models even better? The secret sauce lies in the science of hyperparameter tuning."}),"\n",(0,t.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsx)(n.p,{children:"Hyperparameter tuning is a crucial step in building effective machine learning models. Hyperparameters are parameters whose values are set before the learning process begins, unlike model parameters which are learned during training. Examples include learning rate and number of layers in a neural network."}),"\n",(0,t.jsx)(n.p,{children:"Tuning these hyperparameters can significantly affect the learning process and the performance of the final model. A well-tuned model can learn more efficiently, generalize better to unseen data, and provide more accurate predictions."}),"\n",(0,t.jsx)(n.p,{children:"However, finding the optimal hyperparameters is not a straightforward task. It involves searching a potentially large space of hyperparameters, which can be computationally expensive and time-consuming. This is where hyperparameter tuning tools, such as KerasTuner, come in. They automate and optimize the search process, making it easier to find the best hyperparameters for your model."}),"\n",(0,t.jsx)(n.h2,{id:"getting-kerastuner-up-and-running",children:"Getting KerasTuner Up and Running"}),"\n",(0,t.jsx)(n.p,{children:"KerasTuner is a Python library for hyperparameter tuning. It's designed to work seamlessly with Keras, a popular high-level neural networks API and a front-runner in the machine learning world."}),"\n",(0,t.jsx)(n.p,{children:"KerasTuner offers several methods to tune the hyperparameters, such as Random Search, Hyperband, and Bayesian Optimization. These methods provide a systematic and efficient way to explore the best hyperparameters for your model. For now, let's focus on getting KerasTuner installed on your machine."}),"\n",(0,t.jsx)(n.p,{children:"Open up your terminal or command prompt and type the following command:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",metastring:'title="Shell"',children:"pip install keras-tuner\n"})}),"\n",(0,t.jsx)(n.p,{children:"Voila! Now, let's verify that everything is installed correctly. You can do this by trying to import KerasTuner in a Python script or notebook:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="Python"',children:"import keras_tuner\n"})}),"\n",(0,t.jsx)(n.p,{children:"If you don't see any error messages, congratulations! You've successfully installed KerasTuner and are ready to embark on your hyperparameter tuning journey."}),"\n",(0,t.jsx)(n.h2,{id:"using-kerastuner-on-a-classification-problem",children:"Using KerasTuner on a Classification Problem"}),"\n",(0,t.jsx)(n.p,{children:"Now that we have KerasTuner installed and ready to go, let's dive into a practical example. We'll be working on a classification problem using a tabular dataset. Our goal is to predict a binary outcome based on a set of features. This is a common task in machine learning, so understanding how to tune hyperparameters for this type of problem will be very useful."}),"\n",(0,t.jsx)(n.p,{children:"For our example, we'll use the famous Titanic dataset from Kaggle. This dataset contains information about the passengers on the Titanic, including their age, sex, passenger class, and whether or not they survived the sinking of the ship."}),"\n",(0,t.jsx)(n.h3,{id:"import-libraries",children:"Import Libraries"}),"\n",(0,t.jsx)(n.p,{children:"First, let's setup additional libraries we'll need for this example:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",metastring:'title="Shell"',children:"pip install -q tensorflow pandas scikit-learn seaborn\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="Python"',children:"import pandas as pd\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nfrom tensorflow import keras\n\nimport keras_tuner\n\n\n# Set random seed for reproducibility\nSEED = 42\n"})}),"\n",(0,t.jsx)(n.h3,{id:"load-and-preprocess-the-data",children:"Load and Preprocess the Data"}),"\n",(0,t.jsx)(n.p,{children:"Then, let's load our data and do some basic preprocessing:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="Python"',children:'# Load the data\ndata = sns.load_dataset("titanic")\n\n# Preprocess the data\ndata = data.dropna()  # remove rows with missing values\ndata = pd.get_dummies(data)  # convert categorical variables to dummy variables\n\n# Split into features and target\nX = data.drop("survived", axis=1)\ny = data["survived"]\n\n# Standardize the features\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# Split into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=SEED\n)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"building-the-model",children:"Building the Model"}),"\n",(0,t.jsx)(n.p,{children:"Now, let's define our model-building function. This function will create a Keras model and is where we specify the hyperparameters we want to tune. For this example, we'll tune the number of units in the first hidden layer and the learning rate:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="Python"',children:'def build_model(hp: keras_tuner.HyperParameters):\n    model = keras.Sequential()\n\n    # Tune the number of units in the first Dense layer\n    # Choose an optimal value between 8-64\n    hp_units = hp.Int("units", min_value=8, max_value=64, step=8)\n\n    model.add(keras.layers.Dense(units=hp_units, activation="relu"))\n    model.add(keras.layers.Dense(1, activation="sigmoid"))\n\n    # Tune the learning rate for the optimizer\n    # Choose an optimal value from 0.01, 0.001, or 0.0001\n    hp_learning_rate = hp.Choice("learning_rate", values=[1e-2, 1e-3, 1e-4])\n\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n        loss=keras.losses.BinaryCrossentropy(from_logits=True),\n        metrics=["accuracy"],\n    )\n\n    return model\n'})}),"\n",(0,t.jsxs)(n.p,{children:["In this function, we use the ",(0,t.jsx)(n.code,{children:"HyperParameters"})," object from KerasTuner to define our hyperparameters. The ",(0,t.jsx)(n.code,{children:"Int"})," method is used to specify a range of integer values, and the ",(0,t.jsx)(n.code,{children:"Choice"})," method is used to specify a list of discrete values."]}),"\n",(0,t.jsx)(n.h3,{id:"defining-the-tuner",children:"Defining the Tuner"}),"\n",(0,t.jsxs)(n.p,{children:["With our model-building function defined, we're now ready to start the hyperparameter search. For this, we'll use the ",(0,t.jsx)(n.code,{children:"BayesianOptimization"})," tuner provided by KerasTuner. This tuner uses Bayesian optimization with an underlying Gaussian process model to select combinations of hyperparameters to train the model and keeps the best performing ones."]}),"\n",(0,t.jsx)(n.p,{children:"Let's see how to set it up:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="Python"',children:'tuner = keras_tuner.BayesianOptimization(\n    build_model,\n    objective="val_accuracy",\n    max_trials=5,\n    seed=SEED,\n    max_retries_per_trial=3,\n    directory="random_search",\n    project_name="titanic",\n    overwrite=True,\n)\n\n# Summary of the search space\ntuner.search_space_summary()\n'})}),"\n",(0,t.jsxs)(n.p,{children:["In the ",(0,t.jsx)(n.code,{children:"BayesianOptimization"})," constructor, we pass our ",(0,t.jsx)(n.code,{children:"build_model"})," function, specify our objective (in this case, we want to maximize validation accuracy), and set the number of trials and retries per trial. The ",(0,t.jsx)(n.code,{children:"max_trials"})," parameter determines the number of different combinations to try. The ",(0,t.jsx)(n.code,{children:"directory"})," and ",(0,t.jsx)(n.code,{children:"project_name"})," parameters are used to specify where the results of the hyperparameter search will be saved."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Search space summary\nDefault search space size: 2\nunits (Int)\n{'default': None, 'conditions': [], 'min_value': 8, 'max_value': 64, 'step': 8, 'sampling': 'linear'}\nlearning_rate (Choice)\n{'default': 0.01, 'conditions': [], 'values': [0.01, 0.001, 0.0001], 'ordered': True}\n"})}),"\n",(0,t.jsx)(n.h3,{id:"hyperparameter-search",children:"Hyperparameter Search"}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"search"})," method is then used to perform the hyperparameter search. We pass our training data and labels, specify the number of epochs to train for, and set aside 20% of the training data for validation. We also use the ",(0,t.jsx)(n.code,{children:"TensorBoard"})," callback to log the results of the search."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="Python"',children:'tuner.search(\n    X_train,\n    y_train,\n    epochs=10,\n    validation_split=0.2,\n    callbacks=[keras.callbacks.TensorBoard("./random_search/tb_logs")],\n)\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Trial 5 Complete [00h 00m 01s]\nval_accuracy: 1.0\n\nBest val_accuracy So Far: 1.0\nTotal elapsed time: 00h 00m 03s\n"})}),"\n",(0,t.jsx)(n.h3,{id:"best-model",children:"Best Model"}),"\n",(0,t.jsx)(n.p,{children:"Once the search is complete, we can retrieve the best hyperparameters and use them to build our final model:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="Python"',children:"# Get the best model\nbest_model = tuner.get_best_models(num_models=1)[0]\nbest_model.build(X_train.shape)\n\n# Summary of the best model\nbest_model.summary()\n"})}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"get_best_models"})," method is used to retrieve the best models from the search."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'Model: "sequential"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #\n=================================================================\n dense (Dense)               (145, 48)                 1488\n\n dense_1 (Dense)             (145, 1)                  49\n\n=================================================================\nTotal params: 1537 (6.00 KB)\nTrainable params: 1537 (6.00 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n'})}),"\n",(0,t.jsx)(n.h3,{id:"summary-of-search-results",children:"Summary of Search Results"}),"\n",(0,t.jsxs)(n.p,{children:["Then, we can use the ",(0,t.jsx)(n.code,{children:"results_summary"})," method to get a summary of the results from the hyperparameter search. Note that the results are sorted by the objective value, which in our case is validation accuracy."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="Python"',children:"tuner.results_summary()\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'Results summary\nResults in search/titanic\nShowing 10 best trials\nObjective(name="val_accuracy", direction="max")\n\nTrial 0 summary\nHyperparameters:\nunits: 48\nlearning_rate: 0.01\nScore: 1.0\n\nTrial 1 summary\nHyperparameters:\nunits: 32\nlearning_rate: 0.01\nScore: 1.0\n\nTrial 3 summary\nHyperparameters:\nunits: 40\nlearning_rate: 0.01\nScore: 1.0\n\nTrial 4 summary\nHyperparameters:\nunits: 64\nlearning_rate: 0.01\nScore: 1.0\n\nTrial 2 summary\nHyperparameters:\nunits: 64\nlearning_rate: 0.001\nScore: 0.8620689511299133\n'})}),"\n",(0,t.jsx)(n.h3,{id:"evaluation",children:"Evaluation"}),"\n",(0,t.jsx)(n.p,{children:"Finally, we can evaluate our best model on the test data:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="Python"',children:'# Evaluate the model on the test data\nscore = best_model.evaluate(X_test, y_test, verbose=0)\n\nprint("Test loss:", score[0])\nprint("Test accuracy:", score[1])\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Test loss: 0.10603184998035431\nTest accuracy: 0.9729729890823364\n"})}),"\n",(0,t.jsx)(n.h3,{id:"analysing-the-results",children:"Analysing the Results"}),"\n",(0,t.jsx)(n.p,{children:"TensorBoard is a powerful tool provided by TensorFlow for visualizing and analyzing machine learning experiments. It can be particularly useful for analyzing the results of hyperparameter tuning with KerasTuner."}),"\n",(0,t.jsx)(n.p,{children:"To launch TensorBoard, open up your terminal or command prompt and type the following command:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",metastring:'title="Shell"',children:"tensorboard --logdir search/tb_logs\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Then, open up your browser and go to ",(0,t.jsx)(n.a,{href:"http://localhost:6006",children:"http://localhost:6006"}),"."]}),"\n",(0,t.jsx)(n.p,{children:"You can view the loss and metrics for each trial. In the screenshot below, we can see the train and validation loss and accuracy for the first trial:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"TensorBoard scalars",src:r(45499).A+"",width:"3840",height:"2160"})}),"\n",(0,t.jsx)(n.p,{children:"You can also view the computation graph for each trial. The computation graph for the first trial is shown below:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"TensorBoard graphs",src:r(49221).A+"",width:"3840",height:"2160"})}),"\n",(0,t.jsxs)(n.p,{children:["There is also an ",(0,t.jsx)(n.code,{children:"HPARAMS"})," tab that allows you to compare the performance of different hyperparameters:"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"TensorBoard hparams table view",src:r(11073).A+"",width:"3840",height:"2160"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"TensorBoard hparams parallel coordinates view",src:r(32348).A+"",width:"3840",height:"2160"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Tensorboard hparams scatter plot matrix view",src:r(92275).A+"",width:"3840",height:"2160"})}),"\n",(0,t.jsx)(n.h2,{id:"pro-tips-for-mastering-kerastuner",children:"Pro Tips for Mastering KerasTuner"}),"\n",(0,t.jsx)(n.p,{children:"By following the tips bellow, you can master KerasTuner and use it to build high-performing models:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Define a Wide Range of Hyperparameters"}),": When defining your hyperparameters, try to cover a wide range of possible values. This will give the tuner more freedom to explore and find the best values. However, be careful not to define too wide a range, as this can make the search space too large and the search process inefficient."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Choose the Right Tuner"}),": KerasTuner offers several tuners, each with its own strengths and weaknesses. RandomSearch is a good starting point because it's simple and efficient. If you have more time and computational resources, consider using Hyperband, which is based on the state-of-the-art Hyperband algorithm. If you have prior knowledge about the distribution of good hyperparameters, consider using BayesianOptimization."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Use Early Stopping"}),": Early stopping is a technique where the training process is stopped when the model's performance on a validation set stops improving. This can save a lot of time and computational resources. KerasTuner integrates well with the EarlyStopping callback from Keras."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Parallelize the Search"}),": If you have access to multiple GPUs or a distributed computing system, you can parallelize the hyperparameter search to make it faster. KerasTuner supports distributed tuning."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Analyze the Results"}),": After the hyperparameter search, take the time to analyze the results. This can give you insights into the problem and the performance of your model. For example, you can plot the distribution of scores for different hyperparameters to see how they affect the performance. You can use TensorBoard for a more interactive analysis."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Retrain the Best Model"}),": After the hyperparameter search, retrain the best model on the entire training set (including the validation set) for the optimal number of epochs. This can give the model a small performance boost."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Keep Experimenting"}),": Don't be afraid to experiment with different hyperparameters, tuners, and tuning strategies."]}),"\n"]}),"\n"]})]})}function c(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}},45499:(e,n,r)=>{r.d(n,{A:()=>a});const a=r.p+"assets/images/kerastuner-scalars-415d77f29f0e05105547a2a896a2bf7d.png"},49221:(e,n,r)=>{r.d(n,{A:()=>a});const a=r.p+"assets/images/kerastuner-graphs-818b74843cb0adcae3883cb7adcc919a.png"},92275:(e,n,r)=>{r.d(n,{A:()=>a});const a=r.p+"assets/images/kerastuner-hparams-scatter-d4e3daad622df56423435869fd7fc3da.png"}}]);