"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[6231],{3905:(e,n,t)=>{t.d(n,{Zo:()=>u,kt:()=>g});var a=t(7294);function i(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function r(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function o(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?r(Object(t),!0).forEach((function(n){i(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,a,i=function(e,n){if(null==e)return{};var t,a,i={},r=Object.keys(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||(i[t]=e[t]);return i}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var l=a.createContext({}),d=function(e){var n=a.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):o(o({},n),e)),t},u=function(e){var n=d(e.components);return a.createElement(l.Provider,{value:n},e.children)},m="mdxType",p={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},c=a.forwardRef((function(e,n){var t=e.components,i=e.mdxType,r=e.originalType,l=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),m=d(t),c=i,g=m["".concat(l,".").concat(c)]||m[c]||p[c]||r;return t?a.createElement(g,o(o({ref:n},u),{},{components:t})):a.createElement(g,o({ref:n},u))}));function g(e,n){var t=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var r=t.length,o=new Array(r);o[0]=c;var s={};for(var l in n)hasOwnProperty.call(n,l)&&(s[l]=n[l]);s.originalType=e,s[m]="string"==typeof e?e:i,o[1]=s;for(var d=2;d<r;d++)o[d]=t[d];return a.createElement.apply(null,o)}return a.createElement.apply(null,t)}c.displayName="MDXCreateElement"},6226:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>r,metadata:()=>s,toc:()=>d});var a=t(7462),i=(t(7294),t(3905));const r={title:"Dealing with Missing Data",tags:["Python","Data Science","Missing Data","Machine Learning","Imputation"]},o="Dealing with Missing Data Like a Pro",s={unversionedId:"eda/missing-data",id:"eda/missing-data",title:"Dealing with Missing Data",description:"Welcome, fellow data enthusiasts and Python aficionados! Today, we embark on a thrilling journey to tackle one of the most common challenges in data science &mdash; missing data! Ah, those pesky gaps in our datasets that can ruin our models and give us headaches. But fear not! With Python as our trusty companion, we shall equip ourselves with the skills to handle missing data. So, fasten your seatbelts and let\u2019s dive in!",source:"@site/docs/eda/missing-data.md",sourceDirName:"eda",slug:"/eda/missing-data",permalink:"/portfolio/docs/eda/missing-data",draft:!1,tags:[{label:"Python",permalink:"/portfolio/docs/tags/python"},{label:"Data Science",permalink:"/portfolio/docs/tags/data-science"},{label:"Missing Data",permalink:"/portfolio/docs/tags/missing-data"},{label:"Machine Learning",permalink:"/portfolio/docs/tags/machine-learning"},{label:"Imputation",permalink:"/portfolio/docs/tags/imputation"}],version:"current",frontMatter:{title:"Dealing with Missing Data",tags:["Python","Data Science","Missing Data","Machine Learning","Imputation"]},sidebar:"tutorialSidebar",previous:{title:"Dirty Data",permalink:"/portfolio/docs/eda/dirty-data"},next:{title:"Polars",permalink:"/portfolio/docs/eda/polars"}},l={},d=[{value:"Understanding the Missing Data Landscape",id:"understanding-the-missing-data-landscape",level:2},{value:"Detecting Missing Data",id:"detecting-missing-data",level:2},{value:"Strategies for Handling Missing Data",id:"strategies-for-handling-missing-data",level:2},{value:"Imputation Techniques",id:"imputation-techniques",level:3},{value:"Forward and Backward Fill",id:"forward-and-backward-fill",level:3},{value:"Predictive Imputation",id:"predictive-imputation",level:3},{value:"K-Nearest Neighbors (KNN)",id:"k-nearest-neighbors-knn",level:3},{value:"Verifying Imputations",id:"verifying-imputations",level:2},{value:"Check for Remaining Missing Values",id:"check-for-remaining-missing-values",level:3},{value:"Compare Distributions",id:"compare-distributions",level:3},{value:"Domain-Specific Imputation",id:"domain-specific-imputation",level:2},{value:"Time-Series Imputation",id:"time-series-imputation",level:3},{value:"Categorical Data Imputation",id:"categorical-data-imputation",level:3},{value:"Wrapping up",id:"wrapping-up",level:2}],u={toc:d},m="wrapper";function p(e){let{components:n,...r}=e;return(0,i.kt)(m,(0,a.Z)({},u,r,{components:n,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"dealing-with-missing-data-like-a-pro"},"Dealing with Missing Data Like a Pro"),(0,i.kt)("p",null,"Welcome, fellow data enthusiasts and Python aficionados! Today, we embark on a thrilling journey to tackle one of the most common challenges in data science ","\u2014"," missing data! Ah, those pesky gaps in our datasets that can ruin our models and give us headaches. But fear not! With Python as our trusty companion, we shall equip ourselves with the skills to handle missing data. So, fasten your seatbelts and let\u2019s dive in!"),(0,i.kt)("h2",{id:"understanding-the-missing-data-landscape"},"Understanding the Missing Data Landscape"),(0,i.kt)("p",null,"Before we don our data superhero capes, let\u2019s take a moment to understand the types of missing data we might encounter in our datasets:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Missing Completely at Random (MCAR):")," Imagine a mischievous data gremlin randomly removing entries from our dataset while we\u2019re not looking. Sneaky, right? Well, these missing values are said to be MCAR, where their absence is unrelated to any variables in our dataset. No patterns, no worries!"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Missing at Random (MAR):")," This type of missing data shows a bit of cunning. Here, the probability of data being missing depends on other observed variables. As if it\u2019s saying, \u201cHey, if you already know X and Y, I\u2019ll decide whether to hide.\u201d"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Missing Not at Random (MNAR):")," These missing values are neither random nor related to the observed variables. They might hide for specific reasons unknown to us, and they\u2019re quite adept at keeping secrets.")),(0,i.kt)("h2",{id:"detecting-missing-data"},"Detecting Missing Data"),(0,i.kt)("p",null,"Now that we\u2019re familiar with the missing data breeds, it\u2019s time to put on our detective hats and find those gaps in our dataset. Our Python magnifying glass and trusty pandas library will come to the rescue:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="Python" showLineNumbers',title:'"Python"',showLineNumbers:!0},'import pandas as pd\n\n# Load your dataset\ndata = pd.DataFrame(\n    {\n        "A": [1, 2, None, 4],\n        "B": [5, None, 7, None],\n        "C": [None, 9, None, 11],\n        "D": [12, 8, 14, 15],\n    }\n)\n\n# Print the dataset\nprint("Original dataset:")\nprint(data)\n')),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"Original dataset:\n     A    B     C   D\n0  1.0  5.0   NaN  12\n1  2.0  NaN   9.0   8\n2  NaN  7.0   NaN  14\n3  4.0  NaN  11.0  15\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="Python" showLineNumbers',title:'"Python"',showLineNumbers:!0},'# Check for missing values in the entire dataset\nprint("\\nMissing values in the entire dataset:")\nprint(data.isnull().sum())\n\n# Identify columns with missing values\nprint("\\nColumns with missing values:")\nprint(data.columns[data.isnull().any()].tolist())\n')),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"Missing values in the entire dataset:\nA    1\nB    2\nC    2\nD    0\ndtype: int64\n\nColumns with missing values:\n['A', 'B', 'C']\n")),(0,i.kt)("p",null,"The ",(0,i.kt)("inlineCode",{parentName:"p"},"isnull().sum()")," command gives us the number of missing values in each column, while ",(0,i.kt)("inlineCode",{parentName:"p"},"data.columns[data.isnull().any()]")," reveals the columns with missing data. Now we know where our data gremlins have been up to their mischief!"),(0,i.kt)("h2",{id:"strategies-for-handling-missing-data"},"Strategies for Handling Missing Data"),(0,i.kt)("h3",{id:"imputation-techniques"},"Imputation Techniques"),(0,i.kt)("p",null,"Missing data can leave unsightly holes in our analysis, but fear not, for we shall artfully fill in these gaps with imputation techniques. One popular method is mean imputation:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="Python" showLineNumbers',title:'"Python"',showLineNumbers:!0},'# Print the original column A\nprint("\\nOriginal column A:")\nprint(data["A"])\n\n# Impute missing values with the column A\'s mean\ndata_imputed = data["A"].fillna(data["A"].mean())\n\n# Print the imputed column A\nprint("\\nImputed column A:")\nprint(data_imputed)\n')),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"Original column A:\n0    1.0\n1    2.0\n2    NaN\n3    4.0\nName: A, dtype: float64\n\nImputed column A:\n0    1.000000\n1    2.000000\n2    2.333333\n3    4.000000\nName: A, dtype: float64\n")),(0,i.kt)("h3",{id:"forward-and-backward-fill"},"Forward and Backward Fill"),(0,i.kt)("p",null,"For sequential data, such as time-series, we can employ forward-fill or backward-fill techniques to propagate the last observed value:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="Python" showLineNumbers',title:'"Python"',showLineNumbers:!0},'# Print the original column B\nprint("\\nOriginal column B:")\nprint(data["B"])\n\n# Forward-fill missing values in column B\ndata_imputed = data["B"].fillna(method="ffill")\n\n# Print the forward-filled column B\nprint("\\nForward-filled column B:")\nprint(data_imputed)\n\n# Backward-fill missing values\ndata_imputed = data["B"].fillna(method="bfill")\n\n# Print the backward-filled column B\nprint("\\nBackward-filled column B:")\nprint(data_imputed)\n')),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"Original column B:\n0    5.0\n1    NaN\n2    7.0\n3    NaN\nName: B, dtype: float64\n\nForward-filled column B:\n0    5.0\n1    5.0\n2    7.0\n3    7.0\nName: B, dtype: float64\n\nBackward-filled column B:\n0    5.0\n1    7.0\n2    7.0\n3    NaN\nName: B, dtype: float64\n")),(0,i.kt)("h3",{id:"predictive-imputation"},"Predictive Imputation"),(0,i.kt)("p",null,"For a touch of magic, we can use machine learning models to predict missing values based on other features:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="Python" showLineNumbers',title:'"Python"',showLineNumbers:!0},'from sklearn.ensemble import RandomForestRegressor\n\n\ndef impute_with_random_forest(df, target_column):\n    # Split data into two sets: with and without missing values in the\n    # target column\n    known = df[df[target_column].notnull()]\n    unknown = df[df[target_column].isnull()]\n\n    # Create the Random Forest model\n    model = RandomForestRegressor()\n\n    # Train the model on known data to predict missing values\n    X = known.drop(columns=[target_column])\n    y = known[target_column]\n    model.fit(X, y)\n\n    # Predict missing values\n    predicted_values = model.predict(unknown.drop(columns=[target_column]))\n\n    # Assign predicted values to missing entries\n    df.loc[df[target_column].isnull(), target_column] = predicted_values\n\n\n# Impute missing values of all columns except C\ndata_imputed = data.copy()\ndata_imputed[["A", "B"]] = data_imputed[["A", "B"]].fillna(\n    data_imputed[["A", "B"]].mean()\n)\n\n# Print the dataset before imputing column C\nprint("Before imputing column C:")\nprint(data_imputed)\n\n# Impute the column C\nimpute_with_random_forest(data_imputed, "C")\n\n# Print the dataset after imputing column C\nprint("\\nAfter imputing column C:")\nprint(data_imputed)\n')),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"Before imputing column C:\n          A    B     C   D\n0  1.000000  5.0   NaN  12\n1  2.000000  6.0   9.0   8\n2  2.333333  7.0   NaN  14\n3  4.000000  6.0  11.0  15\n\nAfter imputing column C:\n          A    B      C   D\n0  1.000000  5.0   9.92  12\n1  2.000000  6.0   9.00   8\n2  2.333333  7.0   9.92  14\n3  4.000000  6.0  11.00  15\n")),(0,i.kt)("h3",{id:"k-nearest-neighbors-knn"},"K-Nearest Neighbors (KNN)"),(0,i.kt)("p",null,"Let\u2019s embrace the spirit of camaraderie and let our friendly neighbors help us fill in the missing values:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="Python" showLineNumbers',title:'"Python"',showLineNumbers:!0},'from sklearn.impute import KNNImputer\n\n# Create KNN imputer\nknn_imputer = KNNImputer(n_neighbors=2)\n\n# Perform imputation on all columns with missing values\narray_imputed = knn_imputer.fit_transform(data)\ndata_imputed = pd.DataFrame(array_imputed, columns=data.columns)\n\n# Print the original dataset\nprint("Before imputation:")\nprint(data)\n\n# Print the imputed dataset\nprint("\\nAfter imputation:")\nprint(data_imputed)\n')),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"Before imputation:\n     A    B     C   D\n0  1.0  5.0   NaN  12\n1  2.0  NaN   9.0   8\n2  NaN  7.0   NaN  14\n3  4.0  NaN  11.0  15\n\nAfter imputation:\n     A    B     C     D\n0  1.0  5.0  10.0  12.0\n1  2.0  6.0   9.0   8.0\n2  2.5  7.0  10.0  14.0\n3  4.0  6.0  11.0  15.0\n")),(0,i.kt)("h2",{id:"verifying-imputations"},"Verifying Imputations"),(0,i.kt)("p",null,"Now that we\u2019ve sprinkled Python magic and conquered missing data, it\u2019s time to ensure our imputations are solid:"),(0,i.kt)("h3",{id:"check-for-remaining-missing-values"},"Check for Remaining Missing Values"),(0,i.kt)("p",null,"Let\u2019s verify if our imputation methods left any missing values lurking:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="Python" showLineNumbers',title:'"Python"',showLineNumbers:!0},"# Verify if any missing values remain\nprint(data_imputed.isnull().sum())\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"A    0\nB    0\nC    0\nD    0\ndtype: int64\n")),(0,i.kt)("h3",{id:"compare-distributions"},"Compare Distributions"),(0,i.kt)("p",null,"For the imputed columns, compare the original and imputed distributions to ensure they\u2019re not drastically altered:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="Python" showLineNumbers',title:'"Python"',showLineNumbers:!0},'# Visualize the distribution of the original and imputed data\ndata.plot(\n    kind="density", title="Original data", ylim=(0, 1), figsize=(4, 2)\n)\n\ndata_imputed.plot(\n    kind="density", title="Imputed data", ylim=(0, 1), figsize=(4, 2)\n);\n')),(0,i.kt)("p",null,(0,i.kt)("img",{src:t(7866).Z,width:"381",height:"220"})),(0,i.kt)("p",null,(0,i.kt)("img",{src:t(41).Z,width:"381",height:"220"})),(0,i.kt)("h2",{id:"domain-specific-imputation"},"Domain-Specific Imputation"),(0,i.kt)("h3",{id:"time-series-imputation"},"Time-Series Imputation"),(0,i.kt)("p",null,"For time-series data, we\u2019ll need a specialized approach. Enter interpolation:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="Python" showLineNumbers',title:'"Python"',showLineNumbers:!0},'# Print the dataset before interpolation\nprint("Before interpolation:")\nprint(data)\n\n# Print the dataset after interpolation\nprint("\\nAfter interpolation:")\nprint(data.interpolate())\n')),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"Before interpolation:\n     A    B     C   D\n0  1.0  5.0   NaN  12\n1  2.0  NaN   9.0   8\n2  NaN  7.0   NaN  14\n3  4.0  NaN  11.0  15\n\nAfter interpolation:\n     A    B     C   D\n0  1.0  5.0   NaN  12\n1  2.0  6.0   9.0   8\n2  3.0  7.0  10.0  14\n3  4.0  7.0  11.0  15\n")),(0,i.kt)("h3",{id:"categorical-data-imputation"},"Categorical Data Imputation"),(0,i.kt)("p",null,"When dealing with categorical data, mode imputation comes to our rescue:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="Python" showLineNumbers',title:'"Python"',showLineNumbers:!0},'# Dataset\ndata_cat = pd.Series(["a", "b", None, "d"] * 2 + ["b"])\n\n# Print the dataset before imputation\nprint("Before imputation:")\nprint(data_cat)\n\n# Find the most frequent category\nmost_frequent = data_cat.mode()[0]\nprint("\\nMost frequent category:", most_frequent)\n\n# Replace missing values with the most frequent category\ndata_cat.fillna(most_frequent, inplace=True)\n\n# Print the dataset after imputation\nprint("\\nAfter imputation:")\nprint(data_cat)\n')),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"Before imputation:\n0       a\n1       b\n2    None\n3       d\n4       a\n5       b\n6    None\n7       d\n8       b\ndtype: object\n\nMost frequent category: b\n\nAfter imputation:\n0    a\n1    b\n2    b\n3    d\n4    a\n5    b\n6    b\n7    d\n8    b\ndtype: object\n")),(0,i.kt)("h2",{id:"wrapping-up"},"Wrapping up"),(0,i.kt)("p",null,"Congratulations, data explorers! You\u2019ve learned the secrets of handling missing data like a seasoned data scientist. Remember, handling missing data is an essential skill in the data scientist\u2019s toolkit, and with Python by your side, you\u2019re well-equipped to conquer any missing value challenges that come your way."),(0,i.kt)("p",null,"Stay curious, keep coding, and let your data-driven dreams soar high!"),(0,i.kt)("p",null,"Happy Python-ing! \ud83d\udc0d\u2728"))}p.isMDXComponent=!0},41:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/missing-imputed-c38c852bdd9daf5a71c93ce2846a811a.png"},7866:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/missing-originial-28bdab55056bd9be1f0050ac21344959.png"}}]);