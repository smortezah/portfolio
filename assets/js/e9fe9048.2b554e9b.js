"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[8397],{3905:(e,t,n)=>{n.d(t,{Zo:()=>d,kt:()=>h});var a=n(67294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function s(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function o(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var l=a.createContext({}),u=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):s(s({},t),e)),n},d=function(e){var t=u(e.components);return a.createElement(l.Provider,{value:t},e.children)},p="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},c=a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,r=e.originalType,l=e.parentName,d=o(e,["components","mdxType","originalType","parentName"]),p=u(n),c=i,h=p["".concat(l,".").concat(c)]||p[c]||m[c]||r;return n?a.createElement(h,s(s({ref:t},d),{},{components:n})):a.createElement(h,s({ref:t},d))}));function h(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var r=n.length,s=new Array(r);s[0]=c;var o={};for(var l in t)hasOwnProperty.call(t,l)&&(o[l]=t[l]);o.originalType=e,o[p]="string"==typeof e?e:i,s[1]=o;for(var u=2;u<r;u++)s[u]=n[u];return a.createElement.apply(null,s)}return a.createElement.apply(null,n)}c.displayName="MDXCreateElement"},80241:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>s,default:()=>m,frontMatter:()=>r,metadata:()=>o,toc:()=>u});var a=n(87462),i=(n(67294),n(3905));const r={title:"Taming Overfitting",tags:["Data Science","Time Series Analysis","Overfitting","Regularization","Crossvalidation"]},s="Taming Overfitting in Time Series Data",o={unversionedId:"time-series/overfit",id:"time-series/overfit",title:"Taming Overfitting",description:"Embarking on the intricate journey of time series data analysis reveals a landscape teeming with insights and complexities. Within these sequences of data lies a treasure trove of trends, fluctuations, and patterns waiting to be unveiled. However, as we delve deeper into the art of deciphering time-bound information, we encounter the hurdle of overfitting&mdash;a phenomenon where models become excessively tailored to the training data, jeopardizing their predictive power on unseen instances. In this exploration, we\u2019ll harness Python\u2019s capabilities to tackle overfitting head-on, delving into techniques such as feature selection, cross-validation, and regularization. By embracing tools and guidance, we\u2019ll master the art of mitigating overfitting\u2019s impact in time series analysis, revealing the true essence of the underlying data patterns.",source:"@site/docs/time-series/overfit.md",sourceDirName:"time-series",slug:"/time-series/overfit",permalink:"/portfolio/docs/time-series/overfit",draft:!1,tags:[{label:"Data Science",permalink:"/portfolio/docs/tags/data-science"},{label:"Time Series Analysis",permalink:"/portfolio/docs/tags/time-series-analysis"},{label:"Overfitting",permalink:"/portfolio/docs/tags/overfitting"},{label:"Regularization",permalink:"/portfolio/docs/tags/regularization"},{label:"Crossvalidation",permalink:"/portfolio/docs/tags/crossvalidation"}],version:"current",frontMatter:{title:"Taming Overfitting",tags:["Data Science","Time Series Analysis","Overfitting","Regularization","Crossvalidation"]},sidebar:"tutorialSidebar",previous:{title:"Time Series",permalink:"/portfolio/docs/time-series/"},next:{title:"Forecasting with sktime",permalink:"/portfolio/docs/time-series/sktime"}},l={},u=[{value:"Feature Selection",id:"feature-selection",level:2},{value:"The Art of Feature Selection",id:"the-art-of-feature-selection",level:3},{value:"Shining the Spotlight with \u201cSelectKBest\u201d",id:"shining-the-spotlight-with-selectkbest",level:3},{value:"Tips for the Journey",id:"tips-for-the-journey",level:3},{value:"Cross-Validation",id:"cross-validation",level:2},{value:"The Essence of Cross-Validation",id:"the-essence-of-cross-validation",level:3},{value:"Guiding Steps",id:"guiding-steps",level:3},{value:"Regularization",id:"regularization",level:2},{value:"Understanding Regularization",id:"understanding-regularization",level:3},{value:"Embracing Lasso and Ridge",id:"embracing-lasso-and-ridge",level:3},{value:"Balancing Act",id:"balancing-act",level:3},{value:"Guiding Steps",id:"guiding-steps-1",level:3},{value:"Conclusion",id:"conclusion",level:2}],d={toc:u},p="wrapper";function m(e){let{components:t,...n}=e;return(0,i.kt)(p,(0,a.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"taming-overfitting-in-time-series-data"},"Taming Overfitting in Time Series Data"),(0,i.kt)("p",null,"Embarking on the intricate journey of time series data analysis reveals a landscape teeming with insights and complexities. Within these sequences of data lies a treasure trove of trends, fluctuations, and patterns waiting to be unveiled. However, as we delve deeper into the art of deciphering time-bound information, we encounter the hurdle of overfitting","\u2014","a phenomenon where models become excessively tailored to the training data, jeopardizing their predictive power on unseen instances. In this exploration, we\u2019ll harness Python\u2019s capabilities to tackle overfitting head-on, delving into techniques such as feature selection, cross-validation, and regularization. By embracing tools and guidance, we\u2019ll master the art of mitigating overfitting\u2019s impact in time series analysis, revealing the true essence of the underlying data patterns."),(0,i.kt)("h2",{id:"feature-selection"},"Feature Selection"),(0,i.kt)("p",null,"In the realm of time series data analysis, the choices we make in selecting features are the bedrock upon which our insights stand. Just as a skilled artisan selects the finest materials for their masterpiece, a thoughtful feature selection process empowers us to distill the essence of our data. With Python as our guiding compass, we navigate through the multitude of attributes, aiming to retain those that wield the most influence on our analysis. In this section, we embark on a journey into the realm of feature selection, exploring techniques that allow us to prune away noise and focus solely on the elements that contribute to the overarching narrative of our time series analysis."),(0,i.kt)("h3",{id:"the-art-of-feature-selection"},"The Art of Feature Selection"),(0,i.kt)("p",null,"Feature selection is an art that blends domain knowledge with data exploration. Instead of throwing every available feature into the mix, we want to curate a set of attributes that truly matter for our analysis. This reduces noise and helps our model focus on the meaningful patterns."),(0,i.kt)("p",null,"In Python, the ",(0,i.kt)("inlineCode",{parentName:"p"},"pandas")," library is our trusty partner for handling data. Here, we have our time series data loaded into a DataFrame named ",(0,i.kt)("inlineCode",{parentName:"p"},"data"),", and targeting a column named ",(0,i.kt)("inlineCode",{parentName:"p"},"close_USD"),"."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="Python" showLineNumbers',title:'"Python"',showLineNumbers:!0},'import pandas as pd\nfrom sklearn.datasets import fetch_openml\n\n# Download the data\ndigital_currency = fetch_openml(\n    "Digital-currency---Time-series", as_frame=True, parser="pandas"\n)\n\n# Convert the data to a dataframe\ndata = (\n    digital_currency\n    .frame\n    .drop(columns=["open_SAR", "high_SAR", "low_SAR", "close_SAR"])\n    .rename(columns={"Unnamed:_0": "date"})\n    .set_index("date")\n)\n\n# Separate features and target\ntarget = "close_USD"\ny = data[target]\nX = data.drop(target, axis=1)\n\n# Print full data\ndata.head()\n')),(0,i.kt)("div",null,(0,i.kt)("table",null,(0,i.kt)("thead",null,(0,i.kt)("tr",null,(0,i.kt)("th",null),(0,i.kt)("th",null,"open_USD"),(0,i.kt)("th",null,"high_USD"),(0,i.kt)("th",null,"low_USD"),(0,i.kt)("th",null,"close_USD"),(0,i.kt)("th",null,"volume")),(0,i.kt)("tr",null,(0,i.kt)("th",null,"date"),(0,i.kt)("th",null),(0,i.kt)("th",null),(0,i.kt)("th",null),(0,i.kt)("th",null),(0,i.kt)("th",null))),(0,i.kt)("tbody",{style:{textAlign:"right"}},(0,i.kt)("tr",null,(0,i.kt)("th",null,"2021-01-30"),(0,i.kt)("td",null,"34246.28"),(0,i.kt)("td",null,"34933.00"),(0,i.kt)("td",null,"32825.00"),(0,i.kt)("td",null,"34218.54"),(0,i.kt)("td",null,"43072")),(0,i.kt)("tr",null,(0,i.kt)("th",null,"2021-01-29"),(0,i.kt)("td",null,"33368.18"),(0,i.kt)("td",null,"38531.90"),(0,i.kt)("td",null,"31915.40"),(0,i.kt)("td",null,"34252.20"),(0,i.kt)("td",null,"231827")),(0,i.kt)("tr",null,(0,i.kt)("th",null,"2021-01-28"),(0,i.kt)("td",null,"30362.19"),(0,i.kt)("td",null,"33783.98"),(0,i.kt)("td",null,"29842.10"),(0,i.kt)("td",null,"33364.86"),(0,i.kt)("td",null,"92621")),(0,i.kt)("tr",null,(0,i.kt)("th",null,"2021-01-27"),(0,i.kt)("td",null,"32464.01"),(0,i.kt)("td",null,"32557.29"),(0,i.kt)("td",null,"29241.72"),(0,i.kt)("td",null,"30366.15"),(0,i.kt)("td",null,"95911")),(0,i.kt)("tr",null,(0,i.kt)("th",null,"2021-01-26"),(0,i.kt)("td",null,"32254.19"),(0,i.kt)("td",null,"32921.88"),(0,i.kt)("td",null,"30837.37"),(0,i.kt)("td",null,"32467.77"),(0,i.kt)("td",null,"84972"))))),(0,i.kt)("h3",{id:"shining-the-spotlight-with-selectkbest"},"Shining the Spotlight with \u201cSelectKBest\u201d"),(0,i.kt)("p",null,"Now that we have our features and target separated, it\u2019s time to identify the most influential features. Python\u2019s ",(0,i.kt)("inlineCode",{parentName:"p"},"sklearn")," library comes to the rescue with its ",(0,i.kt)("inlineCode",{parentName:"p"},"SelectKBest")," class, which selects the top ",(0,i.kt)("span",{parentName:"p",className:"math math-inline"},(0,i.kt)("span",{parentName:"span",className:"katex"},(0,i.kt)("span",{parentName:"span",className:"katex-mathml"},(0,i.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,i.kt)("semantics",{parentName:"math"},(0,i.kt)("mrow",{parentName:"semantics"},(0,i.kt)("mi",{parentName:"mrow"},"k")),(0,i.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"k")))),(0,i.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,i.kt)("span",{parentName:"span",className:"base"},(0,i.kt)("span",{parentName:"span",className:"strut",style:{height:"0.6944em"}}),(0,i.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.03148em"}},"k")))))," features based on a scoring function. For univariate time series regression tasks, we often use the ",(0,i.kt)("inlineCode",{parentName:"p"},"f_regression")," scoring function, which measures the correlation between each feature and the target."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="Python" showLineNumbers',title:'"Python"',showLineNumbers:!0},"from sklearn.feature_selection import SelectKBest, f_regression\n\n# Select top k features using f_regression\nk = 2 # You can adjust this number based on your domain knowledge\nselector = SelectKBest(score_func=f_regression, k=k)\nX_selected = selector.fit_transform(X, y)\n")),(0,i.kt)("p",null,"The ",(0,i.kt)("inlineCode",{parentName:"p"},"X_selected")," now holds the top ",(0,i.kt)("span",{parentName:"p",className:"math math-inline"},(0,i.kt)("span",{parentName:"span",className:"katex"},(0,i.kt)("span",{parentName:"span",className:"katex-mathml"},(0,i.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,i.kt)("semantics",{parentName:"math"},(0,i.kt)("mrow",{parentName:"semantics"},(0,i.kt)("mi",{parentName:"mrow"},"k"),(0,i.kt)("mo",{parentName:"mrow"},"="),(0,i.kt)("mn",{parentName:"mrow"},"5")),(0,i.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"k=5")))),(0,i.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,i.kt)("span",{parentName:"span",className:"base"},(0,i.kt)("span",{parentName:"span",className:"strut",style:{height:"0.6944em"}}),(0,i.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.03148em"}},"k"),(0,i.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2778em"}}),(0,i.kt)("span",{parentName:"span",className:"mrel"},"="),(0,i.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2778em"}})),(0,i.kt)("span",{parentName:"span",className:"base"},(0,i.kt)("span",{parentName:"span",className:"strut",style:{height:"0.6444em"}}),(0,i.kt)("span",{parentName:"span",className:"mord"},"5")))))," features that have shown the strongest correlation with your target variable. These features are the stars of the show, contributing meaningfully to your analysis while keeping the noise in check."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="Python" showLineNumbers',title:'"Python"',showLineNumbers:!0},'# Indices of top k features\ntop_indices = selector.get_support(indices=True)\n\n# Top k feature names\ntop_features = selector.feature_names_in_[top_indices]\n\n# Top k scores\ntop_scores = selector.scores_[top_indices]\n\n# Print the names and scores of top k features\nprint(f"{\'Feature\':<10} Score")\nprint(f"{\'-------\':<10} ---------")\nfor feature, score in sorted(zip(top_features, top_scores)):\n    print(f"{feature:<10} {score:.2f}")\n')),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"Feature    Score\n-------    ---------\nhigh_USD   343836.02\nlow_USD    283910.10\n")),(0,i.kt)("h3",{id:"tips-for-the-journey"},"Tips for the Journey"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Domain Knowledge:")," Don\u2019t underestimate your domain expertise. Features that make sense in your specific context are more likely to be relevant."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Feature Importance:")," After selecting features, you can access the selected feature scores using ",(0,i.kt)("inlineCode",{parentName:"li"},"selector.scores_")," and the indices of the selected features using ",(0,i.kt)("inlineCode",{parentName:"li"},"selector.get_support(indices=True)"),"."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Iterate and Refine:")," Feature selection is an iterative process. Test your model\u2019s performance with different sets of features and observe the impact.")),(0,i.kt)("p",null,"With Python, we can gracefully narrow down our feature set, allowing our models to shine on the stage of time series analysis. So, let\u2019s curate our features thoughtfully and lead our models to waltz with the rhythm of the data!"),(0,i.kt)("h2",{id:"cross-validation"},"Cross-Validation"),(0,i.kt)("p",null,"In the enchanting world of time series data analysis, ensuring your model\u2019s performance is as smooth as a waltz requires a special technique: cross-validation. We need to validate our models using a variety of data windows to ensure they\u2019re ready for the grand performance."),(0,i.kt)("h3",{id:"the-essence-of-cross-validation"},"The Essence of Cross-Validation"),(0,i.kt)("p",null,"Cross-validation is like having multiple dress rehearsals before the big show. Instead of relying on a single train-test split, we divide our time series data into multiple segments and use them as both training and testing sets. This approach ensures our model\u2019s performance isn\u2019t biased by the choice of a particular training window."),(0,i.kt)("p",null,"Python offers us a gem called ",(0,i.kt)("inlineCode",{parentName:"p"},"TimeSeriesSplit")," from the ",(0,i.kt)("inlineCode",{parentName:"p"},"sklearn")," library, custom-made for time series data."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="Python" showLineNumbers',title:'"Python"',showLineNumbers:!0},'from sklearn.model_selection import TimeSeriesSplit, cross_validate\nfrom sklearn.linear_model import LinearRegression\n\n# Splitting the data into train and test sets\ntrain_size = int(X_selected.shape[0] * 0.8)\nX_train, y_train = X_selected[:train_size], y[:train_size]\nX_test, y_test = X_selected[train_size:], y[train_size:]\n\n# Splitting the train set into train and validation sets\ntscv = TimeSeriesSplit(n_splits=5)  # You can adjust the number of splits\n\n# Model\nmodel = LinearRegression()\n\n# Cross-validation\ncv = cross_validate(\n    model,\n    X_train,\n    y_train,\n    cv=tscv,\n    scoring=[\n        "neg_root_mean_squared_error",\n        "r2",\n        "neg_mean_absolute_percentage_error"\n    ],\n)\n\n# Printing the results\nprint("RMSE")\nfor i, val in enumerate(cv["test_neg_root_mean_squared_error"]):\n    print(f"Fold {i+1}: {-val:.3f}")\nprint(f"Mean: {-cv[\'test_neg_root_mean_squared_error\'].mean():.3f}")\n')),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"RMSE\nFold 1: 140.891\nFold 2: 202.352\nFold 3: 128.289\nFold 4: 219.695\nFold 5: 76.540\nMean: 153.554\n")),(0,i.kt)("p",null,"By training and validating our model across multiple windows, we gain a more comprehensive understanding of its performance and its ability to adapt to different patterns in the data."),(0,i.kt)("h3",{id:"guiding-steps"},"Guiding Steps"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Sensible Splits:")," Consider the nature of your data and choose the number of splits accordingly. Smaller windows are suitable for short-term trends, while larger windows capture long-term patterns."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Metrics and Insights:")," Track performance metrics like root mean squared error for each validation round. Analyze these metrics to identify if your model is consistently performing well or if it struggles with certain patterns."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Model Evolution:")," If you notice varying performance across different splits, it\u2019s a cue to fine-tune your model\u2019s parameters or explore alternative algorithms.")),(0,i.kt)("p",null,"By harmonizing model performance through cross-validation, we create a performance that\u2019s both consistent and adaptable."),(0,i.kt)("h2",{id:"regularization"},"Regularization"),(0,i.kt)("p",null,"In the intricate landscape of time series data analysis, maintaining a delicate equilibrium between complexity and precision is essential. Models that become overly intricate risk losing sight of the underlying trends, while overly simplistic models might overlook crucial nuances. Here, regularization emerges as a powerful ally. Python equips us with the tools to temper the exuberance of our models by gently constraining their parameters. As we delve into the world of regularization, we\u2019ll uncover the nuances of Lasso and Ridge techniques, striking the right balance that ensures our models capture the essence of time series patterns without succumbing to overfitting\u2019s allure."),(0,i.kt)("h3",{id:"understanding-regularization"},"Understanding Regularization"),(0,i.kt)("p",null,"Regularization is like adding just the right amount of discipline to our models. It\u2019s a technique that prevents them from memorizing the training data\u2019s noise and instead encourages them to focus on the meaningful patterns. Python offers two partners for this endeavor: ",(0,i.kt)("inlineCode",{parentName:"p"},"Lasso")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"Ridge")," regression."),(0,i.kt)("h3",{id:"embracing-lasso-and-ridge"},"Embracing Lasso and Ridge"),(0,i.kt)("p",null,"Lasso and Ridge are like the yin and yang of regularization. Lasso, short for \u201cLeast Absolute Shrinkage and Selection Operator,\u201d has a knack for driving some coefficients to exactly zero, effectively performing feature selection. Ridge, on the other hand, gently nudges coefficients towards zero without making them exactly zero. Both methods help to prevent overfitting and enhance generalization."),(0,i.kt)("p",null,"Let\u2019s take a spin with Python:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="Python" showLineNumbers',title:'"Python"',showLineNumbers:!0},'from sklearn.linear_model import Lasso, Ridge\n\n# Create Lasso and Ridge models\nlasso = Lasso(alpha=0.1, max_iter=3000, random_state=42)\nridge = Ridge(alpha=0.1, random_state=42)\n\n# Fit models to training data\nlasso.fit(X_train, y_train)\nridge.fit(X_train, y_train)\n\n# Print the coefficients and intercepts from both models\nprint("Lasso")\nprint(f"Coefficients: {lasso.coef_}")\nprint(f"Intercept: {lasso.intercept_:.4f}\\n")\n\nprint("Ridge")\nprint(f"Coefficients: {ridge.coef_}")\nprint(f"Intercept: {ridge.intercept_:.4f}")\n')),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"Lasso\nCoefficients: [0.53359161 0.4748104 ]\nIntercept: -56.7733\n\nRidge\nCoefficients: [0.53359067 0.47481144]\nIntercept: -56.7737\n")),(0,i.kt)("h3",{id:"balancing-act"},"Balancing Act"),(0,i.kt)("p",null,"The key to using regularization effectively lies in tuning the hyperparameter ",(0,i.kt)("inlineCode",{parentName:"p"},"alpha"),". Smaller alpha values make the regularization term milder, while larger values increase its impact. By finding the right balance you ensure that your model doesn\u2019t become too rigid or too permissive."),(0,i.kt)("h3",{id:"guiding-steps-1"},"Guiding Steps"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Alpha Tuning:")," Experiment with different alpha values to find the one that works best for your data. Tools like cross-validation can guide you in this exploration."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Feature Insights:")," Regularization can drive some coefficients to zero, effectively removing features. This helps in identifying truly significant features."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Trade-offs:")," Understand that regularization might slightly decrease your model\u2019s training performance in exchange for enhanced generalization on new data.")),(0,i.kt)("p",null,"The following code finds the best alpha, using the Ridge estimator:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="Python" showLineNumbers',title:'"Python"',showLineNumbers:!0},'import math\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\n\n# Initializations\nbest_alpha = 1.0\nbest_mse = math.inf\n\n# Create a list of alphas to test against\nalpha_values = np.linspace(0.1, 1.0, 10)\n\nfor alpha in alpha_values:\n    # Model\n    model = Ridge(alpha=alpha, random_state=42)\n    # Fit\n    model.fit(X_train, y_train)\n    # Predict\n    y_pred = model.predict(X_test)\n    # Evaluate\n    mse = mean_squared_error(y_test, y_pred)\n    # Update results if a better one is achieved\n    if mse < best_mse:\n        best_alpha = alpha\n        best_mse = mse\n\nprint(f"Best alpha: {best_alpha}")\n')),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"Best alpha: 0.1\n")),(0,i.kt)("p",null,"Python, with its Lasso and Ridge partners, equips us to strike the right balance between complexity and simplicity. By guiding our models with gentle restraint, we create analyses that resonate with the true underlying patterns. So, let\u2019s embrace regularization, let the models glide with precision, and achieve an analysis that captivates with both grace and control."),(0,i.kt)("h2",{id:"conclusion"},"Conclusion"),(0,i.kt)("p",null,"As the curtain falls on our journey through preventing overfitting in time series data, remember that your techniques should evolve with your data. From elegant feature selection to the harmonious dance of cross-validation, and the graceful regularization, Python equips us with the tools we need to keep overfitting at bay."),(0,i.kt)("p",null,"Happy coding!"))}m.isMDXComponent=!0}}]);