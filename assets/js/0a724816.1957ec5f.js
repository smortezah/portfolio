"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[5583],{13016:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>s,metadata:()=>r,toc:()=>c});var t=a(85893),i=a(11151);const s={title:"Image Classification App",description:"A deep learning model for image classification",tags:["Data Science","Machine Learning","Image Classification","PyTorch","Streamlit"]},o="How to make an image classification app",r={id:"computer-vision/ants-bees-classification",title:"Image Classification App",description:"A deep learning model for image classification",source:"@site/docs/computer-vision/ants-bees-classification.md",sourceDirName:"computer-vision",slug:"/computer-vision/ants-bees-classification",permalink:"/portfolio/docs/computer-vision/ants-bees-classification",draft:!1,unlisted:!1,tags:[{label:"Data Science",permalink:"/portfolio/docs/tags/data-science"},{label:"Machine Learning",permalink:"/portfolio/docs/tags/machine-learning"},{label:"Image Classification",permalink:"/portfolio/docs/tags/image-classification"},{label:"PyTorch",permalink:"/portfolio/docs/tags/py-torch"},{label:"Streamlit",permalink:"/portfolio/docs/tags/streamlit"}],version:"current",frontMatter:{title:"Image Classification App",description:"A deep learning model for image classification",tags:["Data Science","Machine Learning","Image Classification","PyTorch","Streamlit"]},sidebar:"tutorialSidebar",previous:{title:"Computer Vision",permalink:"/portfolio/docs/computer-vision/"},next:{title:"Data Structures",permalink:"/portfolio/docs/data-structure/"}},l={},c=[{value:"tl;dr",id:"tldr",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Setup",id:"setup",level:2},{value:"Data",id:"data",level:2},{value:"Download the dataset",id:"download-the-dataset",level:2},{value:"Data augmentation and normalisation",id:"data-augmentation-and-normalisation",level:2},{value:"Visualize sample images",id:"visualize-sample-images",level:2},{value:"Train",id:"train",level:2},{value:"ConvNet as fixed feature extractor",id:"convnet-as-fixed-feature-extractor",level:2},{value:"Train and evaluate",id:"train-and-evaluate",level:2},{value:"App",id:"app",level:2}];function d(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",img:"img",p:"p",pre:"pre",strong:"strong",...(0,i.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"how-to-make-an-image-classification-app",children:"How to make an image classification app"}),"\n",(0,t.jsx)(n.h2,{id:"tldr",children:"tl;dr"}),"\n",(0,t.jsx)(n.p,{children:"We will build an app, using deep learning, that can classify (ants and bees) images with an accuracy of 96%."}),"\n",(0,t.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsxs)(n.p,{children:["In this tutorial, we deploy a deep learning model for image classification using transfer learning. The problem that we are going to solve is to classify images of ",(0,t.jsx)(n.em,{children:"ants"})," and ",(0,t.jsx)(n.em,{children:"bees"}),". For that, we use PyTorch to train the model and Streamlit to provide a UI to interact with the model."]}),"\n",(0,t.jsx)(n.h2,{id:"setup",children:"Setup"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="Python"',children:"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport torchvision\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\nimport time\nimport copy\nimport pathlib\nimport requests\nimport zipfile\n"})}),"\n",(0,t.jsx)(n.h2,{id:"data",children:"Data"}),"\n",(0,t.jsxs)(n.p,{children:["We will download 120 training images\u2014a subset of ",(0,t.jsx)(n.a,{href:"https://www.image-net.org/",children:"ImageNet"}),"\u2014for each of ants and bees and also, 75 validation images for each class. If we were going to do the classification from scratch, we would need to collect a lot more data, but since we are using transfer learning, we can get away with a small dataset."]}),"\n",(0,t.jsx)(n.admonition,{type:"note",children:(0,t.jsx)(n.p,{children:"The dataset requires 45.2 MB of disk space."})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="Python"',children:'def download(url, path: pathlib.Path, unzip=False, remove_zipped=False):\n    """Download a file from url to path and potentially unzip it.\n\n    Args:\n        url (str): url to download from\n        path (pathlib.Path): path to download to\n        unzip (bool, optional): unzip the file. Defaults to False.\n        remove_zipped (bool, optional): remove the zipped file. Defaults to\n            False.\n    """\n    # create path if not exists\n    if not path.is_file():\n        path.parent.mkdir(parents=True, exist_ok=True)\n\n    # add zip extension\n    if unzip:\n        path = path.with_suffix(".zip")\n\n    # download\n    r = requests.get(url, stream=True)\n    total_len = int(r.headers.get("content-length"))\n    chunk_size = 1024\n    count = 0\n    with open(path, "wb") as f:\n        for chunk in r.iter_content(chunk_size=chunk_size):\n            if chunk:\n                f.write(chunk)\n                count += 1\n                print(\n                    f"Downloaded {count*chunk_size/total_len*100.0:.1f}%",\n                    end="\\r",\n                )\n\n    # unzip\n    if unzip:\n        with zipfile.ZipFile(path, "r") as zip_ref:\n            zip_ref.extractall(path.parent)\n\n    # remove zipped file\n    if remove_zipped:\n        path.unlink()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"download-the-dataset",children:"Download the dataset"}),"\n",(0,t.jsx)(n.p,{children:"Download the dataset and extract it to the current directory:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="Python"',children:'url = "https://download.pytorch.org/tutorial/hymenoptera_data.zip"\ndata_dir = pathlib.Path("hymenoptera_data")\n\nif not data_dir.is_dir():\n    download(url, data_dir, unzip=True, remove_zipped=True)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"data-augmentation-and-normalisation",children:"Data augmentation and normalisation"}),"\n",(0,t.jsx)(n.p,{children:"Data augmentation and normalisation is done for training, and only data normalisation is performed for validation."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="Python"',children:'prep_img_mean = [0.485, 0.456, 0.406]\nprep_img_std = [0.229, 0.224, 0.225]\n\ndata_transforms = {\n    "train": transforms.Compose(\n        [\n            transforms.RandomResizedCrop(224),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=prep_img_mean, std=prep_img_std),\n        ]\n    ),\n    "val": transforms.Compose(\n        [\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=prep_img_mean, std=prep_img_std),\n        ]\n    ),\n}\n\nimage_datasets = {\n    x: torchvision.datasets.ImageFolder(\n        (data_dir / x).as_posix(), data_transforms[x]\n    )\n    for x in ["train", "val"]\n}\ndataloaders = {\n    x: torch.utils.data.DataLoader(\n        image_datasets[x], batch_size=4, shuffle=True, num_workers=4\n    )\n    for x in ["train", "val"]\n}\ndataset_sizes = {x: len(image_datasets[x]) for x in ["train", "val"]}\nclass_names = image_datasets["train"].classes\n\ndevice = torch.device(\n    "cuda:0"\n    if torch.cuda.is_available()\n    else "mps"\n    if torch.backends.mps.is_available()\n    else "cpu"\n)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"visualize-sample-images",children:"Visualize sample images"}),"\n",(0,t.jsx)(n.p,{children:"Here we show some sample training images for ants and bees."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="Python"',children:'def imshow(inp, title=None):\n    """Imshow for Tensor.\n\n    Args:\n        inp (Tensor): Tensor of shape (C, H, W) to plot.\n        title (str): Title of the plot.\n    """\n    mean = np.array(prep_img_mean)\n    std = np.array(prep_img_std)\n    inp = inp.numpy().transpose((1, 2, 0))\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    if title:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\n\n# get a batch of training data\ninputs, classes = next(iter(dataloaders["train"]))\n\n# make a grid from batch\nout = torchvision.utils.make_grid(inputs)\n\nimshow(out, title=[class_names[x] for x in classes])\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:a(22292).Z+"",width:"552",height:"190"})}),"\n",(0,t.jsx)(n.h2,{id:"train",children:"Train"}),"\n",(0,t.jsx)(n.p,{children:"The following is a generic function to train a model."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="Python"',children:'def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n    """Train a model.\n\n    Args:\n        model: model to train\n        criterion: loss function\n        optimizer: optimizer\n        scheduler: learning rate scheduler\n        num_epochs: number of epochs to train for\n    Returns:\n        model: trained model\n    """\n    since = time.time()\n\n    best_model_weights = copy.deepcopy(model.state_dict())\n    best_accuracy = 0.0\n\n    for epoch in range(num_epochs):\n        print(f"Epoch {epoch+1}/{num_epochs}")\n        print("-" * 10)\n\n        # each epoch has a training and validation phase\n        for phase in ["train", "val"]:\n            if phase == "train":\n                model.train()  # set model to training mode\n            else:\n                model.eval()  # set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # iterate over data\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == "train"):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n\n                    # backward + optimize only if in training phase\n                    if phase == "train":\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            if phase == "train":\n                scheduler.step()\n\n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects.float() / dataset_sizes[phase]\n\n            print(\n                f"{phase}\\tloss: {epoch_loss:.3f}, accuracy: {epoch_acc:.3f}"\n            )\n\n            # deep copy the model\n            if phase == "val" and epoch_acc > best_accuracy:\n                best_accuracy = epoch_acc\n                best_model_weights = copy.deepcopy(model.state_dict())\n\n        print()\n\n    time_elapsed = time.time() - since\n    print(\n        "Training completed in "\n        f"{time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s"\n    )\n    print(f"Best val accuracy: {best_accuracy:.3f}")\n\n    # load best model weights\n    model.load_state_dict(best_model_weights)\n    return model\n'})}),"\n",(0,t.jsx)(n.h2,{id:"convnet-as-fixed-feature-extractor",children:"ConvNet as fixed feature extractor"}),"\n",(0,t.jsx)(n.p,{children:"Here, we freeze all the convolutional neural network (ConvNet) except the final fully connected layer."}),"\n",(0,t.jsx)(n.admonition,{type:"note",children:(0,t.jsx)(n.p,{children:"the regnet_x_3_2gf model weights need 15.3 MB of disk space."})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="Python"',children:"model_conv = torchvision.models.regnet_x_3_2gf(\n    weights=torchvision.models.RegNet_X_3_2GF_Weights.IMAGENET1K_V2\n)\nfor param in model_conv.parameters():\n    param.requires_grad = False\n\n# parameters of newly constructed modules have requires_grad=True by default\nnum_features = model_conv.fc.in_features\nmodel_conv.fc = nn.Linear(num_features, len(class_names))\n\nmodel_conv = model_conv.to(device)\n\n# loss function\ncriterion = nn.CrossEntropyLoss()\n\n# only parameters of final layer are being optimized\noptimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n\n# decay learning rate (LR) by a factor of gamma=0.1 every step_size=3 epochs\nexp_lr_scheduler = optim.lr_scheduler.StepLR(\n    optimizer_conv, step_size=3, gamma=0.1\n)\n"})}),"\n",(0,t.jsx)(n.h2,{id:"train-and-evaluate",children:"Train and evaluate"}),"\n",(0,t.jsx)(n.p,{children:"In this mode, gradients aren\u2019t computed for most of the network, but forward is computed."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="Python"',children:"model_conv = train_model(\n    model=model_conv,\n    criterion=criterion,\n    optimizer=optimizer_conv,\n    scheduler=exp_lr_scheduler,\n    num_epochs=6,\n)\n"})}),"\n",(0,t.jsx)(n.p,{children:"which results in"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Epoch 1/6\n----------\ntrain loss: 0.526, accuracy: 0.795\nval   loss: 0.343, accuracy: 0.948\n\nEpoch 2/6\n----------\ntrain loss: 0.324, accuracy: 0.889\nval   loss: 0.344, accuracy: 0.935\n\nEpoch 3/6\n----------\ntrain loss: 0.372, accuracy: 0.832\nval   loss: 0.235, accuracy: 0.948\n\nEpoch 4/6\n----------\ntrain loss: 0.355, accuracy: 0.848\nval   loss: 0.206, accuracy: 0.961\n\nEpoch 5/6\n----------\ntrain loss: 0.284, accuracy: 0.922\nval   loss: 0.220, accuracy: 0.922\n\nEpoch 6/6\n----------\ntrain loss: 0.319, accuracy: 0.857\nval   loss: 0.257, accuracy: 0.954\n\nTraining completed in 4m 44s\nBest val accuracy: 0.961\n"})}),"\n",(0,t.jsx)(n.p,{children:"As can be seen, the best validation accuracy is 96% \ud83d\udc4d."}),"\n",(0,t.jsx)(n.p,{children:"Save the model for later deployment in Streamlit. It requires 57.6 MB of disk space on my MacBook M1 machine."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="Python"',children:'saved_model_name = "model_img_class.pt"\n\ntorch.save(model_conv, saved_model_name)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"app",children:"App"}),"\n",(0,t.jsx)(n.p,{children:"Here is the fun part!"}),"\n",(0,t.jsx)(n.p,{children:"We make an app, using Streamlit, to interact with the model we just trained."}),"\n",(0,t.jsxs)(n.p,{children:["You can create a file named ",(0,t.jsx)(n.code,{children:"app.py"})," and insert the following code into it, or if you\u2019re using Jupyter Notebook, insert ",(0,t.jsx)(n.code,{children:"%%writefile app.py"})," right before the first line and press Enter."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="Python"',children:'import streamlit as st\nimport torch\nfrom torchvision import transforms\nfrom PIL import Image\nimport argparse\n\nCLASS_NAMES = ["ants", "bees"]\nMODEL_NAME = "model_img_class.pt"\n\n\ndef predict(model_name, img_path):\n    # load the model\n    model = torch.load(model_name, map_location="cpu")\n\n    # preprocess the image\n    prep_img_mean = [0.485, 0.456, 0.406]\n    prep_img_std = [0.229, 0.224, 0.225]\n    transform = transforms.Compose(\n        [\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=prep_img_mean, std=prep_img_std),\n        ]\n    )\n    image = Image.open(img_path)\n    preprocessed_image = transform(image).unsqueeze(0)\n\n    # predict the class\n    model.eval()\n    output = model(preprocessed_image)\n    pred_idx = torch.argmax(output, dim=1)\n    predicted_class = CLASS_NAMES[pred_idx]\n    return predicted_class\n\n\ndef create_app(model_name):\n    # title\n    st.title("Image Classification App")\n\n    # file uploader\n    uploaded_file = st.file_uploader(\n        "Choose an image to classify", type=["jpg", "jpeg", "png"]\n    )\n    if uploaded_file is not None:\n        st.write("")\n\n        # predict the class\n        predicted_class = predict(model_name, uploaded_file)\n\n        col_left, col_right = st.columns(2)\n\n        # the Predict button with the predicted class\n        with col_left:\n            if st.button("Predict"):\n                emoji = ":honeybee:" if predicted_class == "bees" else ":ant:"\n                st.markdown(f"## {predicted_class}  {emoji}")\n\n        # display the image\n        with col_right:\n            image = Image.open(uploaded_file)\n            st.image(image)\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        "--model", type=str, help="model name", default=MODEL_NAME\n    )\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == "__main__":\n    args = parse_args()\n    create_app(model_name=args.model)\n'})}),"\n",(0,t.jsx)(n.p,{children:"Now, we got all the required code and data and can run the app with Streamlit."}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsx)(n.p,{children:"Streamlit is an open-source Python library that makes it easy to create and share beautiful, custom web apps for machine learning and data science. In just a few minutes you can build and deploy powerful data apps."}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["Running the app, with the following command in terminal, will open a browser tab on ",(0,t.jsx)(n.strong,{children:"localhost:8501"}),"."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",metastring:'title="Shell"',children:"streamlit run app.py\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:a(9448).Z+"",width:"1047",height:"1006"})})]})}function p(e={}){const{wrapper:n}={...(0,i.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},9448:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/ants-app-8f5a9eac5d0cf1725b5ed7eb8d7661fb.png"},22292:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/ants-sample-6aff115743c90585e0a16806034bce4e.png"},11151:(e,n,a)=>{a.d(n,{Z:()=>r,a:()=>o});var t=a(67294);const i={},s=t.createContext(i);function o(e){const n=t.useContext(s);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);