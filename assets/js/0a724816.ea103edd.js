"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[5583],{3905:(e,n,t)=>{t.d(n,{Zo:()=>c,kt:()=>h});var a=t(7294);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function i(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function o(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?i(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,a,r=function(e,n){if(null==e)return{};var t,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)t=i[a],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)t=i[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var l=a.createContext({}),p=function(e){var n=a.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):o(o({},n),e)),t},c=function(e){var n=p(e.components);return a.createElement(l.Provider,{value:n},e.children)},m="mdxType",d={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},u=a.forwardRef((function(e,n){var t=e.components,r=e.mdxType,i=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),m=p(t),u=r,h=m["".concat(l,".").concat(u)]||m[u]||d[u]||i;return t?a.createElement(h,o(o({ref:n},c),{},{components:t})):a.createElement(h,o({ref:n},c))}));function h(e,n){var t=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var i=t.length,o=new Array(i);o[0]=u;var s={};for(var l in n)hasOwnProperty.call(n,l)&&(s[l]=n[l]);s.originalType=e,s[m]="string"==typeof e?e:r,o[1]=s;for(var p=2;p<i;p++)o[p]=t[p];return a.createElement.apply(null,o)}return a.createElement.apply(null,t)}u.displayName="MDXCreateElement"},1509:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>i,metadata:()=>s,toc:()=>p});var a=t(7462),r=(t(7294),t(3905));const i={title:"Image Classification App",tags:["Data Science","Machine Learning","Image Classification","PyTorch","Streamlit"]},o="How to make an image classification app",s={unversionedId:"computer-vision/ants-bees-classification",id:"computer-vision/ants-bees-classification",title:"Image Classification App",description:"tl;dr",source:"@site/docs/computer-vision/ants-bees-classification.md",sourceDirName:"computer-vision",slug:"/computer-vision/ants-bees-classification",permalink:"/portfolio/docs/computer-vision/ants-bees-classification",draft:!1,tags:[{label:"Data Science",permalink:"/portfolio/docs/tags/data-science"},{label:"Machine Learning",permalink:"/portfolio/docs/tags/machine-learning"},{label:"Image Classification",permalink:"/portfolio/docs/tags/image-classification"},{label:"PyTorch",permalink:"/portfolio/docs/tags/py-torch"},{label:"Streamlit",permalink:"/portfolio/docs/tags/streamlit"}],version:"current",frontMatter:{title:"Image Classification App",tags:["Data Science","Machine Learning","Image Classification","PyTorch","Streamlit"]},sidebar:"tutorialSidebar",previous:{title:"Fraud Detection",permalink:"/portfolio/docs/anomaly-detection/fraud-detection"},next:{title:"Data Format",permalink:"/portfolio/docs/data-format/"}},l={},p=[{value:"tl;dr",id:"tldr",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Setup",id:"setup",level:2},{value:"Data",id:"data",level:2},{value:"Download the dataset",id:"download-the-dataset",level:2},{value:"Data augmentation and normalisation",id:"data-augmentation-and-normalisation",level:2},{value:"Visualize sample images",id:"visualize-sample-images",level:2},{value:"Train",id:"train",level:2},{value:"ConvNet as fixed feature extractor",id:"convnet-as-fixed-feature-extractor",level:2},{value:"Train and evaluate",id:"train-and-evaluate",level:2},{value:"App",id:"app",level:2}],c={toc:p},m="wrapper";function d(e){let{components:n,...i}=e;return(0,r.kt)(m,(0,a.Z)({},c,i,{components:n,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"how-to-make-an-image-classification-app"},"How to make an image classification app"),(0,r.kt)("h2",{id:"tldr"},"tl;dr"),(0,r.kt)("p",null,"We will build an app, using deep learning, that can classify (ants and bees) images with an accuracy of 96%."),(0,r.kt)("h2",{id:"introduction"},"Introduction"),(0,r.kt)("p",null,"In this tutorial, we deploy a deep learning model for image classification using transfer learning. The problem that we are going to solve is to classify images of ",(0,r.kt)("em",{parentName:"p"},"ants")," and ",(0,r.kt)("em",{parentName:"p"},"bees"),". For that, we use PyTorch to train the model and Streamlit to provide a UI to interact with the model."),(0,r.kt)("h2",{id:"setup"},"Setup"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="Python" showLineNumbers',title:'"Python"',showLineNumbers:!0},"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport torchvision\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\nimport time\nimport copy\nimport pathlib\nimport requests\nimport zipfile\n")),(0,r.kt)("h2",{id:"data"},"Data"),(0,r.kt)("p",null,"We will download 120 training images","\u2014","a subset of ",(0,r.kt)("a",{parentName:"p",href:"https://www.image-net.org/"},"ImageNet"),"\u2014","for each of ants and bees and also, 75 validation images for each class. If we were going to do the classification from scratch, we would need to collect a lot more data, but since we are using transfer learning, we can get away with a small dataset."),(0,r.kt)("admonition",{type:"note"},(0,r.kt)("p",{parentName:"admonition"},"The dataset requires 45.2 MB of disk space.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="Python" showLineNumbers',title:'"Python"',showLineNumbers:!0},'def download(url, path: pathlib.Path, unzip=False, remove_zipped=False):\n    """Download a file from url to path and potentially unzip it.\n\n    Args:\n        url (str): url to download from\n        path (pathlib.Path): path to download to\n        unzip (bool, optional): unzip the file. Defaults to False.\n        remove_zipped (bool, optional): remove the zipped file. Defaults to\n            False.\n    """\n    # create path if not exists\n    if not path.is_file():\n        path.parent.mkdir(parents=True, exist_ok=True)\n\n    # add zip extension\n    if unzip:\n        path = path.with_suffix(".zip")\n\n    # download\n    r = requests.get(url, stream=True)\n    total_len = int(r.headers.get("content-length"))\n    chunk_size = 1024\n    count = 0\n    with open(path, "wb") as f:\n        for chunk in r.iter_content(chunk_size=chunk_size):\n            if chunk:\n                f.write(chunk)\n                count += 1\n                print(\n                    f"Downloaded {count*chunk_size/total_len*100.0:.1f}%",\n                    end="\\r",\n                )\n\n    # unzip\n    if unzip:\n        with zipfile.ZipFile(path, "r") as zip_ref:\n            zip_ref.extractall(path.parent)\n\n    # remove zipped file\n    if remove_zipped:\n        path.unlink()\n')),(0,r.kt)("h2",{id:"download-the-dataset"},"Download the dataset"),(0,r.kt)("p",null,"Download the dataset and extract it to the current directory:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="Python" showLineNumbers',title:'"Python"',showLineNumbers:!0},'url = "https://download.pytorch.org/tutorial/hymenoptera_data.zip"\ndata_dir = pathlib.Path("hymenoptera_data")\n\nif not data_dir.is_dir():\n    download(url, data_dir, unzip=True, remove_zipped=True)\n')),(0,r.kt)("h2",{id:"data-augmentation-and-normalisation"},"Data augmentation and normalisation"),(0,r.kt)("p",null,"Data augmentation and normalisation is done for training, and only data normalisation is performed for validation."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="Python" showLineNumbers',title:'"Python"',showLineNumbers:!0},'prep_img_mean = [0.485, 0.456, 0.406]\nprep_img_std = [0.229, 0.224, 0.225]\n\ndata_transforms = {\n    "train": transforms.Compose(\n        [\n            transforms.RandomResizedCrop(224),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=prep_img_mean, std=prep_img_std),\n        ]\n    ),\n    "val": transforms.Compose(\n        [\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=prep_img_mean, std=prep_img_std),\n        ]\n    ),\n}\n\nimage_datasets = {\n    x: torchvision.datasets.ImageFolder(\n        (data_dir / x).as_posix(), data_transforms[x]\n    )\n    for x in ["train", "val"]\n}\ndataloaders = {\n    x: torch.utils.data.DataLoader(\n        image_datasets[x], batch_size=4, shuffle=True, num_workers=4\n    )\n    for x in ["train", "val"]\n}\ndataset_sizes = {x: len(image_datasets[x]) for x in ["train", "val"]}\nclass_names = image_datasets["train"].classes\n\ndevice = torch.device(\n    "cuda:0"\n    if torch.cuda.is_available()\n    else "mps"\n    if torch.backends.mps.is_available()\n    else "cpu"\n)\n')),(0,r.kt)("h2",{id:"visualize-sample-images"},"Visualize sample images"),(0,r.kt)("p",null,"Here we show some sample training images for ants and bees."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="Python" showLineNumbers',title:'"Python"',showLineNumbers:!0},'def imshow(inp, title=None):\n    """Imshow for Tensor.\n\n    Args:\n        inp (Tensor): Tensor of shape (C, H, W) to plot.\n        title (str): Title of the plot.\n    """\n    mean = np.array(prep_img_mean)\n    std = np.array(prep_img_std)\n    inp = inp.numpy().transpose((1, 2, 0))\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    if title:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\n\n# get a batch of training data\ninputs, classes = next(iter(dataloaders["train"]))\n\n# make a grid from batch\nout = torchvision.utils.make_grid(inputs)\n\nimshow(out, title=[class_names[x] for x in classes])\n')),(0,r.kt)("p",null,(0,r.kt)("img",{src:t(2292).Z,width:"552",height:"190"})),(0,r.kt)("h2",{id:"train"},"Train"),(0,r.kt)("p",null,"The following is a generic function to train a model."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="Python" showLineNumbers',title:'"Python"',showLineNumbers:!0},'def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n    """Train a model.\n\n    Args:\n        model: model to train\n        criterion: loss function\n        optimizer: optimizer\n        scheduler: learning rate scheduler\n        num_epochs: number of epochs to train for\n    Returns:\n        model: trained model\n    """\n    since = time.time()\n\n    best_model_weights = copy.deepcopy(model.state_dict())\n    best_accuracy = 0.0\n\n    for epoch in range(num_epochs):\n        print(f"Epoch {epoch+1}/{num_epochs}")\n        print("-" * 10)\n\n        # each epoch has a training and validation phase\n        for phase in ["train", "val"]:\n            if phase == "train":\n                model.train()  # set model to training mode\n            else:\n                model.eval()  # set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # iterate over data\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == "train"):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n\n                    # backward + optimize only if in training phase\n                    if phase == "train":\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            if phase == "train":\n                scheduler.step()\n\n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects.float() / dataset_sizes[phase]\n\n            print(\n                f"{phase}\\tloss: {epoch_loss:.3f}, accuracy: {epoch_acc:.3f}"\n            )\n\n            # deep copy the model\n            if phase == "val" and epoch_acc > best_accuracy:\n                best_accuracy = epoch_acc\n                best_model_weights = copy.deepcopy(model.state_dict())\n\n        print()\n\n    time_elapsed = time.time() - since\n    print(\n        "Training completed in "\n        f"{time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s"\n    )\n    print(f"Best val accuracy: {best_accuracy:.3f}")\n\n    # load best model weights\n    model.load_state_dict(best_model_weights)\n    return model\n')),(0,r.kt)("h2",{id:"convnet-as-fixed-feature-extractor"},"ConvNet as fixed feature extractor"),(0,r.kt)("p",null,"Here, we freeze all the convolutional neural network (ConvNet) except the final fully connected layer."),(0,r.kt)("admonition",{type:"note"},(0,r.kt)("p",{parentName:"admonition"},"the regnet_x_3_2gf model weights need 15.3 MB of disk space.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="Python" showLineNumbers',title:'"Python"',showLineNumbers:!0},"model_conv = torchvision.models.regnet_x_3_2gf(\n    weights=torchvision.models.RegNet_X_3_2GF_Weights.IMAGENET1K_V2\n)\nfor param in model_conv.parameters():\n    param.requires_grad = False\n\n# parameters of newly constructed modules have requires_grad=True by default\nnum_features = model_conv.fc.in_features\nmodel_conv.fc = nn.Linear(num_features, len(class_names))\n\nmodel_conv = model_conv.to(device)\n\n# loss function\ncriterion = nn.CrossEntropyLoss()\n\n# only parameters of final layer are being optimized\noptimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n\n# decay learning rate (LR) by a factor of gamma=0.1 every step_size=3 epochs\nexp_lr_scheduler = optim.lr_scheduler.StepLR(\n    optimizer_conv, step_size=3, gamma=0.1\n)\n")),(0,r.kt)("h2",{id:"train-and-evaluate"},"Train and evaluate"),(0,r.kt)("p",null,"In this mode, gradients aren\u2019t computed for most of the network, but forward is computed."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="Python" showLineNumbers',title:'"Python"',showLineNumbers:!0},"model_conv = train_model(\n    model=model_conv,\n    criterion=criterion,\n    optimizer=optimizer_conv,\n    scheduler=exp_lr_scheduler,\n    num_epochs=6,\n)\n")),(0,r.kt)("p",null,"which results in"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"Epoch 1/6\n----------\ntrain loss: 0.526, accuracy: 0.795\nval   loss: 0.343, accuracy: 0.948\n\nEpoch 2/6\n----------\ntrain loss: 0.324, accuracy: 0.889\nval   loss: 0.344, accuracy: 0.935\n\nEpoch 3/6\n----------\ntrain loss: 0.372, accuracy: 0.832\nval   loss: 0.235, accuracy: 0.948\n\nEpoch 4/6\n----------\ntrain loss: 0.355, accuracy: 0.848\nval   loss: 0.206, accuracy: 0.961\n\nEpoch 5/6\n----------\ntrain loss: 0.284, accuracy: 0.922\nval   loss: 0.220, accuracy: 0.922\n\nEpoch 6/6\n----------\ntrain loss: 0.319, accuracy: 0.857\nval   loss: 0.257, accuracy: 0.954\n\nTraining completed in 4m 44s\nBest val accuracy: 0.961\n")),(0,r.kt)("p",null,"As can be seen, the best validation accuracy is 96% \ud83d\udc4d."),(0,r.kt)("p",null,"Save the model for later deployment in Streamlit. It requires 57.6 MB of disk space on my MacBook M1 machine."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="Python" showLineNumbers',title:'"Python"',showLineNumbers:!0},'saved_model_name = "model_img_class.pt"\n\ntorch.save(model_conv, saved_model_name)\n')),(0,r.kt)("h2",{id:"app"},"App"),(0,r.kt)("p",null,"Here is the fun part!"),(0,r.kt)("p",null,"We make an app, using Streamlit, to interact with the model we just trained."),(0,r.kt)("p",null,"You can create a file named ",(0,r.kt)("inlineCode",{parentName:"p"},"app.py")," and insert the following code into it, or if you\u2019re using Jupyter Notebook, insert ",(0,r.kt)("inlineCode",{parentName:"p"},"%%writefile app.py")," right before the first line and press Enter."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="Python" showLineNumbers',title:'"Python"',showLineNumbers:!0},'import streamlit as st\nimport torch\nfrom torchvision import transforms\nfrom PIL import Image\nimport argparse\n\nCLASS_NAMES = ["ants", "bees"]\nMODEL_NAME = "model_img_class.pt"\n\n\ndef predict(model_name, img_path):\n    # load the model\n    model = torch.load(model_name, map_location="cpu")\n\n    # preprocess the image\n    prep_img_mean = [0.485, 0.456, 0.406]\n    prep_img_std = [0.229, 0.224, 0.225]\n    transform = transforms.Compose(\n        [\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=prep_img_mean, std=prep_img_std),\n        ]\n    )\n    image = Image.open(img_path)\n    preprocessed_image = transform(image).unsqueeze(0)\n\n    # predict the class\n    model.eval()\n    output = model(preprocessed_image)\n    pred_idx = torch.argmax(output, dim=1)\n    predicted_class = CLASS_NAMES[pred_idx]\n    return predicted_class\n\n\ndef create_app(model_name):\n    # title\n    st.title("Image Classification App")\n\n    # file uploader\n    uploaded_file = st.file_uploader(\n        "Choose an image to classify", type=["jpg", "jpeg", "png"]\n    )\n    if uploaded_file is not None:\n        st.write("")\n\n        # predict the class\n        predicted_class = predict(model_name, uploaded_file)\n\n        col_left, col_right = st.columns(2)\n\n        # the Predict button with the predicted class\n        with col_left:\n            if st.button("Predict"):\n                emoji = ":honeybee:" if predicted_class == "bees" else ":ant:"\n                st.markdown(f"## {predicted_class}  {emoji}")\n\n        # display the image\n        with col_right:\n            image = Image.open(uploaded_file)\n            st.image(image)\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        "--model", type=str, help="model name", default=MODEL_NAME\n    )\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == "__main__":\n    args = parse_args()\n    create_app(model_name=args.model)\n')),(0,r.kt)("p",null,"Now, we got all the required code and data and can run the app with Streamlit."),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"Streamlit is an open-source Python library that makes it easy to create and share beautiful, custom web apps for machine learning and data science. In just a few minutes you can build and deploy powerful data apps.")),(0,r.kt)("p",null,"Running the app, with the following command in terminal, will open a browser tab on ",(0,r.kt)("strong",{parentName:"p"},"localhost:8501"),"."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="Shell"',title:'"Shell"'},"streamlit run app.py\n")),(0,r.kt)("p",null,(0,r.kt)("img",{src:t(9448).Z,width:"1047",height:"1006"})))}d.isMDXComponent=!0},9448:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/ants-app-8f5a9eac5d0cf1725b5ed7eb8d7661fb.png"},2292:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/ants-sample-6aff115743c90585e0a16806034bce4e.png"}}]);