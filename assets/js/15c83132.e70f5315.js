"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[8146],{3905:(e,n,t)=>{t.d(n,{Zo:()=>p,kt:()=>h});var o=t(7294);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function i(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);n&&(o=o.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,o)}return t}function r(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?i(Object(t),!0).forEach((function(n){a(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function l(e,n){if(null==e)return{};var t,o,a=function(e,n){if(null==e)return{};var t,o,a={},i=Object.keys(e);for(o=0;o<i.length;o++)t=i[o],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(o=0;o<i.length;o++)t=i[o],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var s=o.createContext({}),c=function(e){var n=o.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):r(r({},n),e)),t},p=function(e){var n=c(e.components);return o.createElement(s.Provider,{value:n},e.children)},d="mdxType",m={inlineCode:"code",wrapper:function(e){var n=e.children;return o.createElement(o.Fragment,{},n)}},f=o.forwardRef((function(e,n){var t=e.components,a=e.mdxType,i=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),d=c(t),f=a,h=d["".concat(s,".").concat(f)]||d[f]||m[f]||i;return t?o.createElement(h,r(r({ref:n},p),{},{components:t})):o.createElement(h,r({ref:n},p))}));function h(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var i=t.length,r=new Array(i);r[0]=f;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l[d]="string"==typeof e?e:a,r[1]=l;for(var c=2;c<i;c++)r[c]=t[c];return o.createElement.apply(null,r)}return o.createElement.apply(null,t)}f.displayName="MDXCreateElement"},5946:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>r,default:()=>m,frontMatter:()=>i,metadata:()=>l,toc:()=>c});var o=t(7462),a=(t(7294),t(3905));const i={title:"ETL pipeline",tags:["AirFlow","Docker","ETL","PostgreSQL","Data Engineering"]},r="Fastest way to implement an ETL pipeline",l={unversionedId:"etl/airflow-docker",id:"etl/airflow-docker",title:"ETL pipeline",description:"In the world of data engineering, Extract, Transform, Load (ETL) pipelines are crucial for collecting, cleaning, and moving data from various sources to a target destination. ETL pipelines are also used to integrate different data sources and systems.",source:"@site/docs/etl/airflow-docker.md",sourceDirName:"etl",slug:"/etl/airflow-docker",permalink:"/portfolio/docs/etl/airflow-docker",draft:!1,tags:[{label:"AirFlow",permalink:"/portfolio/docs/tags/air-flow"},{label:"Docker",permalink:"/portfolio/docs/tags/docker"},{label:"ETL",permalink:"/portfolio/docs/tags/etl"},{label:"PostgreSQL",permalink:"/portfolio/docs/tags/postgre-sql"},{label:"Data Engineering",permalink:"/portfolio/docs/tags/data-engineering"}],version:"current",frontMatter:{title:"ETL pipeline",tags:["AirFlow","Docker","ETL","PostgreSQL","Data Engineering"]},sidebar:"tutorialSidebar",previous:{title:"Extract, Transform, Load",permalink:"/portfolio/docs/etl/"},next:{title:"Miscellaneous",permalink:"/portfolio/docs/misc/"}},s={},c=[{value:"What is Airflow\u2122?",id:"what-is-airflow",level:2},{value:"What is Docker Compose?",id:"what-is-docker-compose",level:2},{value:"How to implement an ETL Pipeline?",id:"how-to-implement-an-etl-pipeline",level:2},{value:"Step 1: Define the Docker Compose file",id:"step-1-define-the-docker-compose-file",level:3},{value:"Step 2: Define the Airflow\u2122 DAG",id:"step-2-define-the-airflow-dag",level:3},{value:"Step 3: Build and start the Docker containers",id:"step-3-build-and-start-the-docker-containers",level:3},{value:"Conclusion",id:"conclusion",level:2}],p={toc:c},d="wrapper";function m(e){let{components:n,...t}=e;return(0,a.kt)(d,(0,o.Z)({},p,t,{components:n,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"fastest-way-to-implement-an-etl-pipeline"},"Fastest way to implement an ETL pipeline"),(0,a.kt)("p",null,"In the world of data engineering, Extract, Transform, Load (ETL) pipelines are crucial for collecting, cleaning, and moving data from various sources to a target destination. ETL pipelines are also used to integrate different data sources and systems."),(0,a.kt)("p",null,"In this article, we will discuss how to implement an ETL pipeline using Airflow","\u2122"," and Docker Compose."),(0,a.kt)("h2",{id:"what-is-airflow"},"What is Airflow","\u2122","?"),(0,a.kt)("p",null,(0,a.kt)("a",{parentName:"p",href:"https://airflow.apache.org/"},"Airflow","\u2122")," is an open-source platform used for creating, scheduling, and monitoring workflows. It is commonly used in data engineering, data science, and machine learning to create and manage complex data pipelines."),(0,a.kt)("p",null,"Airflow","\u2122"," allows users to define their workflows as directed acyclic graphs (DAGs), where each node represents a task and the edges represent the dependencies between tasks. These DAGs can be scheduled to run at specific times or triggered by certain events, and Airflow","\u2122"," provides a rich set of features to manage these workflows, including task retries, email notifications, and integrations with various external systems."),(0,a.kt)("p",null,"Airflow","\u2122"," has become popular in the data engineering and data science communities due to its flexibility, scalability, and extensibility. It has a large and active community that has created a wide range of plugins and integrations with other tools and systems."),(0,a.kt)("h2",{id:"what-is-docker-compose"},"What is Docker Compose?"),(0,a.kt)("p",null,(0,a.kt)("a",{parentName:"p",href:"https://docs.docker.com/compose/"},"Docker Compose")," is a tool that enables developers to define and run multi-container Docker applications. It simplifies the process of managing multiple containers by providing a unified interface to start, stop, and manage all of the containers in an application. With Docker Compose, developers can define their application\u2019s services, networks, and volumes in a ",(0,a.kt)("a",{parentName:"p",href:"https://yaml.org/"},"YAML")," file and use the Docker Compose CLI to start and stop the application."),(0,a.kt)("p",null,"Docker Compose is particularly useful for microservices architectures, where multiple containers are used to build a single application. It allows developers to define environment variables, set up dependencies between containers, and manage networking between the containers. By using Docker Compose, developers can easily develop, test, and deploy complex applications that require multiple containers, and the tool can also be used to automate the deployment of applications to production environments. Overall, Docker Compose is a powerful and popular tool in the DevOps community, providing a streamlined way to manage multi-container Docker applications."),(0,a.kt)("h2",{id:"how-to-implement-an-etl-pipeline"},"How to implement an ETL Pipeline?"),(0,a.kt)("p",null,"To implement an ETL pipeline using Airflow","\u2122"," and Docker Compose, we will follow the steps below:"),(0,a.kt)("h3",{id:"step-1-define-the-docker-compose-file"},"Step 1: Define the Docker Compose file"),(0,a.kt)("p",null,"The first step is to define the Docker Compose file, which defines the services that will be used in the ETL pipeline. The fastest way of doing this is to download the ",(0,a.kt)("a",{parentName:"p",href:"https://airflow.apache.org/docs/apache-airflow/2.5.1/docker-compose.yaml"},"official file")," (version 2.5.1)."),(0,a.kt)("p",null,"Now, create a new directory etl and download the Docker Compose file into this directory:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="Shell"',title:'"Shell"'},"mkdir etl;\ncd etl;\n\ncurl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.5.1/docker-compose.yaml'\n")),(0,a.kt)("p",null,"Here is the contents of the ",(0,a.kt)("inlineCode",{parentName:"p"},"docker-compose.yaml")," file we just downloaded:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-YAML",metastring:'title="YAML"',title:'"YAML"'},'version: \'3\'\nx-airflow-common:\n  &airflow-common\n  # In order to add custom dependencies or upgrade provider packages you can use your extended image.\n  # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml\n  # and uncomment the "build" line below, Then run `docker-compose build` to build the images.\n  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.5.1}\n  # build: .\n  environment:\n    &airflow-common-env\n    AIRFLOW__CORE__EXECUTOR: CeleryExecutor\n    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow\n    # For backward compatibility, with Airflow <2.3\n    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow\n    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow\n    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0\n    AIRFLOW__CORE__FERNET_KEY: \'\'\n    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: \'true\'\n    AIRFLOW__CORE__LOAD_EXAMPLES: \'true\'\n    AIRFLOW__API__AUTH_BACKENDS: \'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session\'\n    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}\n  volumes:\n    - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags\n    - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs\n    - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins\n  user: "${AIRFLOW_UID:-50000}:0"\n  depends_on:\n    &airflow-common-depends-on\n    redis:\n      condition: service_healthy\n    postgres:\n      condition: service_healthy\n\nservices:\n  postgres:\n    image: postgres:13\n    environment:\n      POSTGRES_USER: airflow\n      POSTGRES_PASSWORD: airflow\n      POSTGRES_DB: airflow\n    volumes:\n      - postgres-db-volume:/var/lib/postgresql/data\n    healthcheck:\n      test: ["CMD", "pg_isready", "-U", "airflow"]\n      interval: 5s\n      retries: 5\n    restart: always\n\n  redis:\n    image: redis:latest\n    expose:\n      - 6379\n    healthcheck:\n      test: ["CMD", "redis-cli", "ping"]\n      interval: 5s\n      timeout: 30s\n      retries: 50\n    restart: always\n\n  airflow-webserver:\n    <<: *airflow-common\n    command: webserver\n    ports:\n      - 8080:8080\n    healthcheck:\n      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]\n      interval: 10s\n      timeout: 10s\n      retries: 5\n    restart: always\n    depends_on:\n      <<: *airflow-common-depends-on\n      airflow-init:\n        condition: service_completed_successfully\n\n  airflow-scheduler:\n    <<: *airflow-common\n    command: scheduler\n    healthcheck:\n      test: ["CMD-SHELL", \'airflow jobs check --job-type SchedulerJob --hostname "$${HOSTNAME}"\']\n      interval: 10s\n      timeout: 10s\n      retries: 5\n    restart: always\n    depends_on:\n      <<: *airflow-common-depends-on\n      airflow-init:\n        condition: service_completed_successfully\n\n  airflow-worker:\n    <<: *airflow-common\n    command: celery worker\n    healthcheck:\n      test:\n        - "CMD-SHELL"\n        - \'celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"\'\n      interval: 10s\n      timeout: 10s\n      retries: 5\n    environment:\n      <<: *airflow-common-env\n      # Required to handle warm shutdown of the celery workers properly\n      # See https://airflow.apache.org/docs/docker-stack/entrypoint.html#signal-propagation\n      DUMB_INIT_SETSID: "0"\n    restart: always\n    depends_on:\n      <<: *airflow-common-depends-on\n      airflow-init:\n        condition: service_completed_successfully\n\n  airflow-triggerer:\n    <<: *airflow-common\n    command: triggerer\n    healthcheck:\n      test: ["CMD-SHELL", \'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"\']\n      interval: 10s\n      timeout: 10s\n      retries: 5\n    restart: always\n    depends_on:\n      <<: *airflow-common-depends-on\n      airflow-init:\n        condition: service_completed_successfully\n\n  airflow-init:\n    <<: *airflow-common\n    entrypoint: /bin/bash\n    # yamllint disable rule:line-length\n    command:\n      - -c\n      - |\n        function ver() {\n          printf "%04d%04d%04d%04d" $${1//./ }\n        }\n        airflow_version=$$(AIRFLOW__LOGGING__LOGGING_LEVEL=INFO && gosu airflow airflow version)\n        airflow_version_comparable=$$(ver $${airflow_version})\n        min_airflow_version=2.2.0\n        min_airflow_version_comparable=$$(ver $${min_airflow_version})\n        if (( airflow_version_comparable < min_airflow_version_comparable )); then\n          echo\n          echo -e "\\033[1;31mERROR!!!: Too old Airflow version $${airflow_version}!\\e[0m"\n          echo "The minimum Airflow version supported: $${min_airflow_version}. Only use this or higher!"\n          echo\n          exit 1\n        fi\n        if [[ -z "${AIRFLOW_UID}" ]]; then\n          echo\n          echo -e "\\033[1;33mWARNING!!!: AIRFLOW_UID not set!\\e[0m"\n          echo "If you are on Linux, you SHOULD follow the instructions below to set "\n          echo "AIRFLOW_UID environment variable, otherwise files will be owned by root."\n          echo "For other operating systems you can get rid of the warning with manually created .env file:"\n          echo "    See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user"\n          echo\n        fi\n        one_meg=1048576\n        mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))\n        cpus_available=$$(grep -cE \'cpu[0-9]+\' /proc/stat)\n        disk_available=$$(df / | tail -1 | awk \'{print $$4}\')\n        warning_resources="false"\n        if (( mem_available < 4000 )) ; then\n          echo\n          echo -e "\\033[1;33mWARNING!!!: Not enough memory available for Docker.\\e[0m"\n          echo "At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))"\n          echo\n          warning_resources="true"\n        fi\n        if (( cpus_available < 2 )); then\n          echo\n          echo -e "\\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\\e[0m"\n          echo "At least 2 CPUs recommended. You have $${cpus_available}"\n          echo\n          warning_resources="true"\n        fi\n        if (( disk_available < one_meg * 10 )); then\n          echo\n          echo -e "\\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\\e[0m"\n          echo "At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))"\n          echo\n          warning_resources="true"\n        fi\n        if [[ $${warning_resources} == "true" ]]; then\n          echo\n          echo -e "\\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\\e[0m"\n          echo "Please follow the instructions to increase amount of resources available:"\n          echo "   https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin"\n          echo\n        fi\n        mkdir -p /sources/logs /sources/dags /sources/plugins\n        chown -R "${AIRFLOW_UID}:0" /sources/{logs,dags,plugins}\n        exec /entrypoint airflow version\n    # yamllint enable rule:line-length\n    environment:\n      <<: *airflow-common-env\n      _AIRFLOW_DB_UPGRADE: \'true\'\n      _AIRFLOW_WWW_USER_CREATE: \'true\'\n      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}\n      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}\n      _PIP_ADDITIONAL_REQUIREMENTS: \'\'\n    user: "0:0"\n    volumes:\n      - ${AIRFLOW_PROJ_DIR:-.}:/sources\n\n  airflow-cli:\n    <<: *airflow-common\n    profiles:\n      - debug\n    environment:\n      <<: *airflow-common-env\n      CONNECTION_CHECK_MAX_COUNT: "0"\n    # Workaround for entrypoint issue. See: https://github.com/apache/airflow/issues/16252\n    command:\n      - bash\n      - -c\n      - airflow\n\n  # You can enable flower by adding "--profile flower" option e.g. docker-compose --profile flower up\n  # or by explicitly targeted on the command line e.g. docker-compose up flower.\n  # See: https://docs.docker.com/compose/profiles/\n  flower:\n    <<: *airflow-common\n    command: celery flower\n    profiles:\n      - flower\n    ports:\n      - 5555:5555\n    healthcheck:\n      test: ["CMD", "curl", "--fail", "http://localhost:5555/"]\n      interval: 10s\n      timeout: 10s\n      retries: 5\n    restart: always\n    depends_on:\n      <<: *airflow-common-depends-on\n      airflow-init:\n        condition: service_completed_successfully\n\nvolumes:\n  postgres-db-volume:\n')),(0,a.kt)("p",null,"The most important services defined in this file are:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"airflow-webserver"),", that provides a web interface for managing workflows (Airflow","\u2122"," DAGs), and is available at http://localhost:8080,"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"airflow-scheduler"),", that is responsible for scheduling and running Airflow","\u2122"," tasks,"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"airflow-worker"),", which executes the tasks given by the scheduler, and"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"postgres"),", that is the PostgreSQL database service for storing the Airflow","\u2122"," metadata and also our dataset. It creates a database named ",(0,a.kt)("inlineCode",{parentName:"li"},"airflow"),".")),(0,a.kt)("p",null,"In summary, this docker-compose file defines the necessary services to run an Airflow","\u2122"," cluster with CeleryExecutor, and PostgreSQL. The services are configured with health checks to ensure they are running correctly."),(0,a.kt)("h3",{id:"step-2-define-the-airflow-dag"},"Step 2: Define the Airflow","\u2122"," DAG"),(0,a.kt)("p",null,"The second step is to define the Airflow","\u2122"," DAG, which defines the tasks that make up the ETL pipeline. In our case, we will define a DAG that performs the following tasks:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Extract data from a CSV file, that is publicly available at ",(0,a.kt)("a",{parentName:"li",href:"https://stats.govt.nz"},"https://stats.govt.nz"),"."),(0,a.kt)("li",{parentName:"ul"},"Transform the data by selecting a few features of it, i.e., ",(0,a.kt)("inlineCode",{parentName:"li"},"Data_value")," and ",(0,a.kt)("inlineCode",{parentName:"li"},"STATUS"),"."),(0,a.kt)("li",{parentName:"ul"},"Load the transformed data into the table ",(0,a.kt)("inlineCode",{parentName:"li"},"data")," in the PostgreSQL database ",(0,a.kt)("inlineCode",{parentName:"li"},"airflow"),".")),(0,a.kt)("p",null,"To do so, create the sub-directory ",(0,a.kt)("inlineCode",{parentName:"p"},"dags")," inside the ",(0,a.kt)("inlineCode",{parentName:"p"},"etl")," directory we created earlier, and also create ",(0,a.kt)("inlineCode",{parentName:"p"},"etl.py")," that will include the DAG."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="Shell"',title:'"Shell"'},"mkdir dags;\ncd dags;\n\ntouch etl.py\n")),(0,a.kt)("p",null,"Then, paste the following self-explanatory code into ",(0,a.kt)("inlineCode",{parentName:"p"},"etl.py"),":"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="Python" showLineNumbers',title:'"Python"',showLineNumbers:!0},'import pandas as pd\nimport pendulum\nfrom airflow.decorators import dag, task\nimport sqlalchemy\n\n# sqlalchemy engine to connect to postgres\nengine = sqlalchemy.create_engine("postgresql://airflow:airflow@postgres:5432")\n\n\n@task()\ndef extract():\n    """Download data and save it to a file."""\n    df = pd.read_csv(\n        "https://stats.govt.nz/assets/Uploads/Business-financial-data/"\n        "Business-financial-data-September-2022-quarter/Download-data/"\n        "business-financial-data-september-2022-quarter-csv.zip"\n    )\n    extracted_path = "raw_data.csv"\n    df.to_csv(extracted_path, index=False)\n    return extracted_path\n\n\n@task()\ndef transform(path):\n    """Transform data and save it to a file."""\n    df = pd.read_csv(path)\n    df = df[["Data_value", "STATUS"]]\n    transformed_path = "data.csv"\n    df.to_csv(transformed_path, index=False)\n    return transformed_path\n\n\n@task()\ndef load(path):\n    """Load data into a database."""\n    df = pd.read_csv(path)\n    table_name = "data"\n    df.to_sql(table_name, con=engine, if_exists="replace", index=False)\n\n    # Query the data to check it was loaded correctly\n    with engine.connect() as connection:\n        query = f"SELECT * FROM {table_name} LIMIT 3"\n        print("Data:", connection.execute(query).fetchall())\n\n\n@dag(\n    dag_id="etl_pipeline",\n    start_date=pendulum.datetime(2023, 3, 10, tz="UTC"),\n)\ndef pipeline():\n    extracted_file = extract()\n    transformed_file = transform(extracted_file)\n    load(transformed_file)\n\n\netl_pipeline = pipeline()\n')),(0,a.kt)("h3",{id:"step-3-build-and-start-the-docker-containers"},"Step 3: Build and start the Docker containers"),(0,a.kt)("p",null,"The final step is to build and start the Docker containers. To do this, navigate to the directory containing the Docker Compose file, i.e., ",(0,a.kt)("inlineCode",{parentName:"p"},"etl"),", and run the following command:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="Shell"',title:'"Shell"'},"cd ..;  # navigate to 'etl', if you're already in the 'dags' directory\n\ndocker compose up -d\n")),(0,a.kt)("p",null,"This command will start the services defined in the ",(0,a.kt)("inlineCode",{parentName:"p"},"docker-compose.yaml")," and let them run in the background, using ",(0,a.kt)("inlineCode",{parentName:"p"},"-d")," option. Note that, this command will create the following sub-directories in ",(0,a.kt)("inlineCode",{parentName:"p"},"etl")," if they don\u2019t already exist:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"./dags"),", to hold the DAG files,"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"./logs"),", that contains logs from task execution and scheduler, and"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"./plugins"),", that can hold ",(0,a.kt)("a",{parentName:"li",href:"https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/plugins.html"},"custom plugins"),".")),(0,a.kt)("p",null,"Once the containers are up and running, you can access the Airflow","\u2122"," webserver by navigating to http://localhost:8080 in your web browser and inserting ",(0,a.kt)("inlineCode",{parentName:"p"},"airflow")," as Username and Password. From there, you can run the DAG."),(0,a.kt)("h2",{id:"conclusion"},"Conclusion"),(0,a.kt)("p",null,"In this article, we have shown how to implement an ETL pipeline using Airflow","\u2122"," and Docker Compose. With these tools, you can easily create and manage complex workflows that extract, transform, and load data from various sources. By using Docker Compose, you can easily deploy your ETL pipeline to different environments, such as cloud infrastructure, making it highly scalable and portable."))}m.isMDXComponent=!0}}]);