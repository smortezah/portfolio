"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[1280],{3905:(e,t,a)=>{a.d(t,{Zo:()=>d,kt:()=>h});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=n.createContext({}),m=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},d=function(e){var t=m(e.components);return n.createElement(l.Provider,{value:t},e.children)},p="mdxType",c={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},u=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),p=m(a),u=r,h=p["".concat(l,".").concat(u)]||p[u]||c[u]||o;return a?n.createElement(h,i(i({ref:t},d),{},{components:a})):n.createElement(h,i({ref:t},d))}));function h(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=a.length,i=new Array(o);i[0]=u;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[p]="string"==typeof e?e:r,i[1]=s;for(var m=2;m<o;m++)i[m]=a[m];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}u.displayName="MDXCreateElement"},6995:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>c,frontMatter:()=>o,metadata:()=>s,toc:()=>m});var n=a(7462),r=(a(7294),a(3905));const o={title:"Data Balancing",tags:["Data Science","Data Imbalance","Resampling","Machine Learning","SMOTE"]},i="Taming the Data Imbalance with Python",s={unversionedId:"eda/data-balancing",id:"eda/data-balancing",title:"Data Balancing",description:"Welcome back to another thrilling ride on the data science rollercoaster. Today, we\u2019re diving headfirst into the wacky world of data balancing&mdash;a crucial skill in your machine learning toolkit. Picture this: you\u2019ve got a dataset that\u2019s as unbalanced as a seesaw with an elephant on one side and a feather on the other. Fear not! We\u2019re about to wield our coding wands and conjure up some enchanting solutions to slay the data imbalance dragon. \ud83d\udc09\ud83d\udd25",source:"@site/docs/eda/data-balancing.md",sourceDirName:"eda",slug:"/eda/data-balancing",permalink:"/portfolio/docs/eda/data-balancing",draft:!1,tags:[{label:"Data Science",permalink:"/portfolio/docs/tags/data-science"},{label:"Data Imbalance",permalink:"/portfolio/docs/tags/data-imbalance"},{label:"Resampling",permalink:"/portfolio/docs/tags/resampling"},{label:"Machine Learning",permalink:"/portfolio/docs/tags/machine-learning"},{label:"SMOTE",permalink:"/portfolio/docs/tags/smote"}],version:"current",frontMatter:{title:"Data Balancing",tags:["Data Science","Data Imbalance","Resampling","Machine Learning","SMOTE"]},sidebar:"tutorialSidebar",previous:{title:"Exploratory Data Analysis",permalink:"/portfolio/docs/eda/"},next:{title:"Dirty Data",permalink:"/portfolio/docs/eda/dirty-data"}},l={},m=[{value:"Taming the Data Asymmetry",id:"taming-the-data-asymmetry",level:2},{value:"Resampling to the Rescue",id:"resampling-to-the-rescue",level:2},{value:"Oversampling Magic",id:"oversampling-magic",level:3},{value:"Undersampling Charms",id:"undersampling-charms",level:3},{value:"SMOTE to the Rescue",id:"smote-to-the-rescue",level:2},{value:"Evaluation Matters!",id:"evaluation-matters",level:2},{value:"The Splitting Spell",id:"the-splitting-spell",level:3},{value:"Cross-Validation",id:"cross-validation",level:3},{value:"Ensemble to the Rescue",id:"ensemble-to-the-rescue",level:2},{value:"Conclusion",id:"conclusion",level:2}],d={toc:m},p="wrapper";function c(e){let{components:t,...a}=e;return(0,r.kt)(p,(0,n.Z)({},d,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"taming-the-data-imbalance-with-python"},"Taming the Data Imbalance with Python"),(0,r.kt)("p",null,"Welcome back to another thrilling ride on the data science rollercoaster. Today, we\u2019re diving headfirst into the wacky world of data balancing","\u2014","a crucial skill in your machine learning toolkit. Picture this: you\u2019ve got a dataset that\u2019s as unbalanced as a seesaw with an elephant on one side and a feather on the other. Fear not! We\u2019re about to wield our coding wands and conjure up some enchanting solutions to slay the data imbalance dragon. \ud83d\udc09\ud83d\udd25"),(0,r.kt)("h2",{id:"taming-the-data-asymmetry"},"Taming the Data Asymmetry"),(0,r.kt)("p",null,"Before we don our coding capes and leap into the heart of data balancing, let\u2019s unravel the enigma that is data imbalance. Imagine you\u2019re at a party where one corner is bustling with a crowd ready to dance the night away, while the other corner seems like a ghost town. That\u2019s what data imbalance looks like in the world of machine learning","\u2014","an unequal distribution of classes that can send your models waltzing off in the wrong direction."),(0,r.kt)("p",null,"But fret not, dear reader, for Python comes to the rescue with its trusty companions ",(0,r.kt)("inlineCode",{parentName:"p"},"numpy"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"pandas"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"scikit-learn"),", and ",(0,r.kt)("inlineCode",{parentName:"p"},"imbalanced-learn"),". Armed with these tools, we\u2019ll load up our dataset and unveil its secrets. It\u2019s like detective work, but with code!"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="Shell"',title:'"Shell"'},"pip install numpy pandas scikit-learn imbalanced-learn\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="Python" showLineNumbers',title:'"Python"',showLineNumbers:!0},'import numpy as np\nimport pandas as pd\n\n# Generate data\ndata = pd.DataFrame()\nseeds = [7, 42, 73, 101]\nn_rows = 200\nfor i, seed in enumerate(seeds):\n    np.random.seed(seed)\n    data[f"feature_{i+1}"] = np.random.randint(0, 10, n_rows)\ndata["target"] = np.random.choice([0, 1, 2], size=n_rows, p=[0.15, 0.30, 0.55])\n\nX = data.drop("target", axis=1)\ny = data["target"]\n\n# The class distribution\nclass_counts = y.value_counts()\nprint(class_counts)\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"target\n2    112\n1     61\n0     27\n")),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"value_counts()")," method gives us a glimpse into the abyss. You see the classes 0, 1 and 2 have 27, 61 and 112 instances, respectively. That\u2019s the data imbalance waving its banner!"),(0,r.kt)("p",null,"But fear not, for we\u2019re just getting started on this quest to conquer the imbalance beast. We\u2019re about to turn the tables and bring equilibrium to this topsy-turvy world. Buckle up","\u2014","we\u2019re diving into the magical realm of resampling in the next enchanting section! \ud83c\udfa9\ud83d\udd0d"),(0,r.kt)("h2",{id:"resampling-to-the-rescue"},"Resampling to the Rescue"),(0,r.kt)("p",null,"Ready to harness the power of Python to tackle that pesky data imbalance? It\u2019s time to roll up your sleeves and dive into the enchanting world of resampling. Think of it as your trusty potion that turns a lopsided dataset into a symphony of balance, all while keeping our charming Python flair intact."),(0,r.kt)("p",null,"Remember, we\u2019re on a mission to restore harmony to the universe of data, where the scales are tipped like a seesaw with a mischievous elephant on one side and a feather on the other."),(0,r.kt)("h3",{id:"oversampling-magic"},"Oversampling Magic"),(0,r.kt)("p",null,"Imagine you\u2019re a party host with a disproportionately long guest list of party animals and a meager smattering of introverts. What do you do? You clone the introverts and suddenly your party is booming! That\u2019s oversampling for you. In Python, our spellcaster of choice is the ",(0,r.kt)("inlineCode",{parentName:"p"},"RandomOverSampler")," from the ",(0,r.kt)("inlineCode",{parentName:"p"},"imbalanced-learn")," library. Let\u2019s weave our magic:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="Python" showLineNumbers',title:'"Python"',showLineNumbers:!0},"from imblearn.over_sampling import RandomOverSampler\n\n# Craft the Oversampling Potion\noversampler = RandomOverSampler(random_state=42)\n\n# Perform the spell: Resample the data\nX_resampled, y_resampled = oversampler.fit_resample(X, y)\n\n# Reveal the new class distribution\nresampled_class_counts = pd.Series(y_resampled).value_counts()\nprint(resampled_class_counts)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"target\n2    112\n1    112\n0    112\n")),(0,r.kt)("h3",{id:"undersampling-charms"},"Undersampling Charms"),(0,r.kt)("p",null,"Now, imagine you\u2019re a zookeeper trying to maintain balance in your animal kingdom. You\u2019ve got a surplus of zebras and a lonely lion sulking in the corner. What do you do? Bid adieu to a few zebras and suddenly the lion\u2019s reign begins! That\u2019s the essence of undersampling. Our Python spellbook reveals the ",(0,r.kt)("inlineCode",{parentName:"p"},"RandomUnderSampler")," from the same magical library. Behold:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="Python" showLineNumbers',title:'"Python"',showLineNumbers:!0},"from imblearn.under_sampling import RandomUnderSampler\n\n# Brew the Undersampling Elixir\nundersampler = RandomUnderSampler(random_state=42)\n\n# Cast the spell: Resample the data\nX_resampled, y_resampled = undersampler.fit_resample(X, y)\n\n# Unveil the new class distribution\nresampled_class_counts = pd.Series(y_resampled).value_counts()\nprint(resampled_class_counts)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"target\n0    27\n1    27\n2    27\n")),(0,r.kt)("p",null,"Who knew battling data imbalance could be so thrilling? With oversampling and undersampling, you\u2019re wielding Python-powered magic that\u2019ll have you dancing with your dataset like never before! But wait, intrepid coder, our adventure is far from over. In the next section, we\u2019ll dive even deeper and explore the art of crafting synthetic samples with SMOTE, an advanced spell that\u2019ll leave your data equilibrium looking like a perfectly choreographed tango! \ud83c\udfa9\ud83c\udfad\ud83d\udc83"),(0,r.kt)("h2",{id:"smote-to-the-rescue"},"SMOTE to the Rescue"),(0,r.kt)("p",null,"You\u2019ve successfully dipped your toes into the waters of resampling, but now it\u2019s time to take a quantum leap into the realm of Synthetic Minority Over-sampling Technique (SMOTE). Imagine a world where you can create magical replicas of your minority class, weaving new instances into the fabric of your dataset. Say goodbye to data imbalance","\u2014","SMOTE is here to weave its intricate spells!"),(0,r.kt)("p",null,"SMOTE is like a master potion maker that crafts synthetic samples, bridging the gap between the majority and minority classes. Think of it as an artist who adds brushstrokes to a canvas to bring balance and harmony. In Python, we summon the mighty ",(0,r.kt)("inlineCode",{parentName:"p"},"SMOTE")," from the ",(0,r.kt)("inlineCode",{parentName:"p"},"imbalanced-learn")," library to perform this act of data alchemy:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="Python" showLineNumbers',title:'"Python"',showLineNumbers:!0},"from imblearn.over_sampling import SMOTE\n\n# Prepare the Cauldron for SMOTE\nsmote = SMOTE(random_state=42)\n\n# Unleash the enchantment: Resample the data\nX_resampled, y_resampled = smote.fit_resample(X, y)\n\n# Reveal the new class distribution\nresampled_class_counts = pd.Series(y_resampled).value_counts()\nprint(resampled_class_counts)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"target\n2    112\n1    112\n0    112\n")),(0,r.kt)("p",null,"Imagine having your own magical forge where you\u2019re crafting instances of your minority class like an artisan blacksmith. With SMOTE, your dataset becomes a masterpiece of balance and representation. You\u2019ve gone beyond traditional resampling and entered the realm of creating entirely new experiences for your models!"),(0,r.kt)("h2",{id:"evaluation-matters"},"Evaluation Matters!"),(0,r.kt)("p",null,"As we tread further into the labyrinth of data balancing, a crucial pit stop awaits us: the realm of evaluation. Just as a knight must test their sword before charging into battle, we must rigorously assess the performance of our resampled models. After all, balance isn\u2019t just about quantity; it\u2019s about quality and effectiveness. So, let\u2019s don our detective hats and scrutinize our models like seasoned data sleuths!"),(0,r.kt)("h3",{id:"the-splitting-spell"},"The Splitting Spell"),(0,r.kt)("p",null,"Before we put our models to the test, we must partition our dataset into training and testing sets. Python has just the incantation we need, courtesy of the ",(0,r.kt)("inlineCode",{parentName:"p"},"train_test_split")," function from the ",(0,r.kt)("inlineCode",{parentName:"p"},"sklearn.model_selection")," module:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="Python" showLineNumbers',title:'"Python"',showLineNumbers:!0},"from sklearn.model_selection import train_test_split\n\n# Split the dataset into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n")),(0,r.kt)("p",null,"With our trusty data division spell, we\u2019ve created training and testing sets that will be instrumental in measuring the prowess of our models."),(0,r.kt)("h3",{id:"cross-validation"},"Cross-Validation"),(0,r.kt)("p",null,"Next up, we summon the art of cross-validation. Just as a wizard practices their spells to perfection, our models need rigorous testing to ensure they\u2019re up to the task. Python provides an instrument called ",(0,r.kt)("inlineCode",{parentName:"p"},"cross_val_score")," to help us wield the power of cross-validation:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="Python" showLineNumbers',title:'"Python"',showLineNumbers:!0},'from sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Create a KNN classifier\nknn = KNeighborsClassifier()\n\n# Perform cross-validation\nscores = cross_val_score(knn, X_train, y_train, cv=5, scoring="f1_macro")\n\n# Display the scores\nprint("Cross-Validation Scores:", [round(score, 3) for score in scores])\nprint("Mean F1-score:", round(scores.mean(), 3))\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"Cross-Validation Scores: [0.378, 0.25, 0.214, 0.197, 0.444]\nMean F1-score: 0.296\n")),(0,r.kt)("p",null,"Here, we\u2019ve used the ",(0,r.kt)("span",{parentName:"p",className:"math math-inline"},(0,r.kt)("span",{parentName:"span",className:"katex"},(0,r.kt)("span",{parentName:"span",className:"katex-mathml"},(0,r.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,r.kt)("semantics",{parentName:"math"},(0,r.kt)("mrow",{parentName:"semantics"},(0,r.kt)("mi",{parentName:"mrow"},"k")),(0,r.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"k")))),(0,r.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,r.kt)("span",{parentName:"span",className:"base"},(0,r.kt)("span",{parentName:"span",className:"strut",style:{height:"0.6944em"}}),(0,r.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.03148em"}},"k"))))),"-NN classifier as our artifact and employed cross-validation to measure its performance. The ",(0,r.kt)("inlineCode",{parentName:"p"},"f1_macro")," scoring metric helps us account for the imbalanced classes, giving us a clearer picture of how well our model performs on both the majority and minority classes."),(0,r.kt)("p",null,"Remember, accurate evaluation is the compass guiding us through the uncharted waters of data balancing. Cross-validation provides a sneak peek into how our models might perform in the real world, helping us fine-tune our strategies and ensure our data equilibrium is on point."),(0,r.kt)("p",null,"But our adventure doesn\u2019t end here! In the next section, we\u2019re going to pull out the big guns","\u2014","ensemble methods that\u2019ll have you cheering for your models like an enchanted audience at a grand wizardry show. Prepare to witness the grand finale of our data balancing saga! \ud83e\uddd9\u200d\u2642\ufe0f\ud83d\udd0d\ud83d\udd2e"),(0,r.kt)("h2",{id:"ensemble-to-the-rescue"},"Ensemble to the Rescue"),(0,r.kt)("p",null,"The time has come for the grand finale of our data balancing odyssey! We\u2019ve delved deep into the art of resampling, crafted synthetic spells with SMOTE, and fine-tuned our models for performance. But now, we\u2019re about to unveil the true masterpiece","\u2014","the ensemble methods that\u2019ll have you applauding your models like a standing ovation at a magical symphony."),(0,r.kt)("p",null,"Ensemble methods are like assembling a league of extraordinary heroes, each with their own unique powers, to tackle a formidable foe. In our case, that foe is the data imbalance monster! Python has bestowed upon us a treasure trove of ensemble algorithms, and we\u2019ll summon the illustrious ",(0,r.kt)("inlineCode",{parentName:"p"},"RandomForestClassifier")," to center stage:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="Python" showLineNumbers',title:'"Python"',showLineNumbers:!0},'from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\n# Split the dataset into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Create the model\nrf_classifier = RandomForestClassifier(random_state=42)\n\n# Train the model\nrf_classifier.fit(X_train, y_train)\n\n# Make predictions\ny_pred = rf_classifier.predict(X_test)\n\n# Assess the performance\nreport = classification_report(y_test, y_pred)\nprint("Classification Report:\\n", report)\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"Classification Report:\n               precision    recall  f1-score   support\n\n           0       0.25      0.25      0.25         4\n           1       0.40      0.12      0.19        16\n           2       0.48      0.75      0.59        20\n\n    accuracy                           0.45        40\n   macro avg       0.38      0.38      0.34        40\nweighted avg       0.43      0.45      0.40        40\n")),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"RandomForestClassifier")," is like a conductor guiding an orchestra of decision trees, harmonizing their efforts to create a powerful, balanced prediction engine. We\u2019ve trained our ensemble hero on the resampled data and used the ",(0,r.kt)("inlineCode",{parentName:"p"},"predict")," function to gaze into the crystal ball and see how well our model performs."),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"classification_report")," function shines a light on the battlefield, revealing precision, recall, and F1-score for each class. It\u2019s like having a mystical seer provide insights into the strengths and weaknesses of your model\u2019s performance."),(0,r.kt)("p",null,"Ensemble methods offer a plethora of options","\u2014","from bagging with Random Forests to boosting with XGBoost. Each has its own flair, but the common thread is their ability to navigate the treacherous waters of imbalanced data, maintaining equilibrium and making accurate predictions."),(0,r.kt)("h2",{id:"conclusion"},"Conclusion"),(0,r.kt)("p",null,"Well, there you have it! We\u2019ve embarked on a journey through the wilds of data imbalance, armed with nothing but our trusty Python spells. We\u2019ve learned to wield the power of resampling, craft synthetic samples, and even tamed the ensemble beasts. So, go forth, balance your data, and conquer those machine learning challenges like the true Python wizards you are! \ud83c\udfa9\ud83d\udc0d"),(0,r.kt)("p",null,"Keep coding! \ud83d\udcbb\u2728"))}c.isMDXComponent=!0}}]);